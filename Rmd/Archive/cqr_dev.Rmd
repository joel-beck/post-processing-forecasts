---
title: "CQR Variants Development"
author: "Joel Beck"
output:
  bookdown::pdf_document2:
    highlight: tango  # pygments
    toc: FALSE
    number_sections: FALSE
    df_print: tibble
    latex_engine: pdflatex
    keep_tex: FALSE
  bookdown::html_document2:
    theme: flatly
    highlight: pygments  # kate
    toc: TRUE
    toc_float: TRUE
    number_sections: FALSE
    df_print: tibble
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE, fig.align = "center",
  out.width = "100%", dpi = 300, fig.asp = 0.618, collapse = TRUE
)
```

```{r libraries}
library(dplyr)
```

```{r}
df <- readr::read_csv(here::here("data_modified", "uk_data_incidences.csv"))
method <- "cqr_multiplicative"
model <- "EuroCOVIDhub-baseline"
location <- "GB"
target_type <- "Cases"
horizon <- 1
quantile <- 0.05
cv_init_training <- 6

mod <- model
t <- target_type
h <- horizon
q <- quantile
l <- location

df <- df |>
  dplyr::filter(
    .data$model == mod & .data$location == l & .data$target_type == t & .data$horizon == h
  )

quantiles_list <- filter_combination(df, model, location, target_type, horizon, quantile)

true_values <- quantiles_list$true_values
# contains lower quantile predictions of 0
quantiles_low <- quantiles_list$quantiles_low
quantiles_high <- quantiles_list$quantiles_high

# code of cqr_multiplicative
scores_list <- compute_scores_multiplicative(true_values, quantiles_low, quantiles_high)

# chooses the MAXIMUM value of score vectors in both cases
# ISSUE: Multiplicative Factor for lower quantiles is much larger than for upper quantiles
# => Adjusted lower quantiles are in some cases LARGER than adjusted upper quantiles
# compare lower_bound and upper_bound below
margin_lower <- compute_margin(scores_list$scores_lower, quantile)
margin_upper <- compute_margin(scores_list$scores_upper, quantile)

list(
  # adjust lower and upper bound with multiplicative margin factor
  margin_lower = margin_lower,
  margin_upper = margin_upper,
  lower_bound = quantiles_low * margin_lower,
  upper_bound = quantiles_high * margin_upper
)
```

# Check Reproducibility

```{r}
# Run this chunk to reproduce old values
# The issue is only contained in the cqr file
# When commenting new fix_quantiles_crossing() function out and replacing new with old cqr file, the result is reproducible !!
# fix_quantiles_crossing() improves cqr result however!!
# Do not forget to add method argument again to update_subset_cqr()
cqr_results_old <- readr::read_rds(here::here("data_results", "uk_cqr.rds"))

devtools::load_all()
cv_init_training <- 0.5
uk_data <- readr::read_csv(
  here::here("data_modified", "uk_data_incidences.csv")
)

complete_models <- uk_data |>
  dplyr::count(model) |>
  dplyr::filter(n == max(n)) |>
  dplyr::pull(model)

df_updated <- update_predictions(
  df = uk_data, methods = "cqr",
  models = complete_models, cv_init_training = cv_init_training, verbose = TRUE
)

cqr_results_new <- df_updated |> collect_predictions()

cqr_results_old |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = c("method"))

cqr_results_new |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = c("method"))
```


```{r}
cqr_results <- readr::read_rds(here::here("data_results", "uk_cqr.rds"))
cqr3_results <- readr::read_rds(here::here("data_results", "uk_cqr3.rds"))
```

```{r}
library(scoringutils)
cqr3_results |>
  extract_training_set() |>
  # scoringutils::eval_forecasts(summarise_by = c("method"))
  score() |>
  summarise_scores(by = c("method"))
# dplyr::arrange(model, interval_score)
```

```{r}
# check if results are equal for old results of cqr method, right now they are not !!
library(dplyr)
sub1 <- cqr_results |> filter(method == "cqr")
sub2 <- cqr3_results |> filter(method == "cqr")

sub1[sub1$prediction != sub2$prediction, ] |> count(target_end_date)
```


# CQR Results for different training-validation splits

```{r}
full_results <- readr::read_rds(here::here("data_results", "uk_cqr3_cv.rds"))

full_results |>
  extract_training_set() |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = c("cv_init_training", "method")) |>
  dplyr::arrange(cv_init_training)
```

```{r}
# training set
df_eval <- eval_methods(full_results, summarise_by = "cv_init_training", training_set = TRUE)
plot_eval(df_eval)
```

```{r}
# validation set
df_eval <- eval_methods(full_results, summarise_by = "cv_init_training")
plot_eval(df_eval)
```
 
 
 






 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
