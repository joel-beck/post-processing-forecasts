---
title: "Testing QSA Methods and Evaluating Optimizers"
author: "Matthias Herp"
output:
  bookdown::html_document2:
    theme: flatly
    highlight: pygments  # kate
    toc: TRUE
    toc_float: TRUE
    toc_depth: 2
    number_sections: FALSE
    df_print: tibble
  bookdown::pdf_document2:
    highlight: tango  # pygments
    toc: FALSE
    number_sections: FALSE
    df_print: tibble
    latex_engine: pdflatex
    keep_tex: FALSE
urlcolor: blue
editor_options: 
  chunk_output_type: inline
---

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE, fig.align = "center",
  out.width = "100%", dpi = 300, fig.asp = 0.618, collapse = TRUE
)
```

```{r libraries, include=FALSE}
devtools::load_all(".")
library(scoringutils)
library(dplyr)
library(patchwork)
library(ggplot2)
```

# Notebook Purpose

We evaluate QSA on testing data in order to see if it acts how we would expect it to. Furthermore we also analyze how different optimizers lead to different QSA results. The main purpose of this notebook is to document our findings in a manner we can reuse when we write our report.

# Testing function

First we define a testing function we can use for creating simulation data and applying QSA to it. It preforms the following steps:
1. Get a subset of original data as template. We allow users to set the number of observations for simulation data. However it is capped at a maximum number of 25 observations, like in the EU Hub data.
2. Changing the quantile predictions to the value of the quantile * 100 * error_factor. We thus change the szenarios by adjustung true values and keep the initial qunatile predictions at these fixed factors to make it simpler. Further we include an error_factor. which is initially at 1 but is allowed to change as a way to define what adjustment QSA should do if we start with optimal quantiles.
3. Change True values as defined by user. The length of the true values defines the number of observations.
4. Runs QSA and returns a plot with the prediction interval to see how qsa adjusted the intervals.

```{r,include=FALSE}
df <- read.csv(here::here("data_modified", "hub_data_2_incidences.csv"))
m <- "epiforecasts-EpiExpert"
l <- "GB"
t <- "Cases"
h <- 1

test <- function(qs = c(0.25, 0.5, 0.75),
                 error_factor = 1,
                 true_values_new = c(rep(50,4), 55, rep(5,5), rep(c(50,5),8),50),
                 methods = "qsa_uniform",
                 cv_init_training = NULL,
                 quantile_plot = 0.25,
                 optim_method = "BFGS",
                 lower_bound_optim = 0, 
                 upper_bound_optim = 5 #,optim_multiple_bounds_brent = FALSE
                 ){
  
  # Get a subset of original data as template 
  # We only make it have the length of the true values (max remains 25)
  subset <- dplyr::filter(df, model == m & location == l 
                          & target_type == t & horizon == h 
                          & quantile %in% qs 
                          & target_end_date %in% sort(unique(df$target_end_date))[0:length(true_values_new)])
  
  # Changing the quantile predictions to the value of the quantile * 100 * error_factor
  subset_generated <- subset
  for (qi in qs) {
    subset_generated <- subset_generated |>
      dplyr::mutate(prediction = replace(
        .data$prediction, .data$quantile == qi,
        values = rep(qi * 100 * error_factor,length(unique(subset$target_end_date)))
      ))
  }
  
  # Changing the true values at each time point (for each quantile the same)
  for (i in seq(0,length(unique(subset_generated$target_end_date))) ) {
    date <- unique(subset_generated$target_end_date)[i]
    value <- true_values_new[i]
    
    subset_generated <- subset_generated |>
      dplyr::mutate(true_value = replace(
        .data$true_value, .data$target_end_date == date,
        values = rep(value,length(unique(subset_generated$quantile)))
      ))
  }
  
  # Run QSA
  df_updated <- update_predictions(subset_generated, methods = methods,
                                   models = m, locations = l, target_types = t,
                                   horizons = h, quantiles = qs,
                                   cv_init_training = cv_init_training, penalty_weight=NULL, 
                                   optim_method = optim_method, 
                                   lower_bound_optim = lower_bound_optim, 
                                   upper_bound_optim = upper_bound_optim,
                                   #optim_multiple_bounds_brent = optim_multiple_bounds_brent,
                                   return_list = TRUE)
  
  # Combine DataFrames so that we can plot results
  df_combined <- df_updated |> collect_predictions()
  
  # Plot results for a specific quantile
  plot_intervals(df_combined, model = m, location = l, target_type = t, quantile = quantile_plot, horizon = h)
  
}
```

## WIS 

Here is a short reminder of the WIS (see Presentation 1):

$$
Score_\alpha(y) = (u-l) + \frac{2}{\alpha} \cdot (l-y) \cdot \mathbf{1} (y \leq l) + \frac{2}{\alpha} \cdot (y-u) \cdot \mathbf{1}(y \geq u)
$$
We define two functions that returns WIS for one interval so the IS actually. It can be used to check what the correct adjustment is. Both give back the same minima. check the tetst_wis notebook to see that both functions do the same.

```{r}
wis_calc <- function(x_vec = seq(0,3,0.1),
                     qs = c(0.25, 0.5, 0.75),
                     true_values_new = c(rep(50,4), 60, rep(20,5))){
  
  wis_vec <- c()
  for (x in x_vec){
    l <- qs[1] * 100 +  (qs[1]-qs[2])*100 * (x - 1)
    u <- qs[3] * 100 +  (qs[3]-qs[2])*100 * (x - 1)
    a <- qs[3] - qs[1]
    
    wis_val <- length(true_values_new) * abs(u-l)
    
    for (obs in true_values_new){
      if (obs > l && obs > u){
        wis_val <- wis_val + 2/a + (obs-u)
      } else if (obs < l && obs < u) {
        wis_val <- wis_val + 2/a* (l-obs)
      }
    }
    wis_vec <- c(wis_vec,wis_val)
  }
  
  return(wis_vec* a / 2)
}

wis_calc_pac <- function(x_vec = seq(-2,2,0.1),
                     qs = c(0.25, 0.5, 0.75),
                     true_values_new = c(rep(50,4), 60, rep(20,5))){
  

  # Get a subset of original data as template 
  # We only make it have the length of the true values (max remains 25)
  subset <- dplyr::filter(df, model == m & location == l 
                          & target_type == t & horizon == h 
                          & quantile %in% qs 
                          & target_end_date %in% sort(unique(df$target_end_date))[0:length(true_values_new)])
  
  # Changing the quantile predictions to the value of the quantile * 100 * error_factor
  subset_generated <- subset
  for (qi in qs) {
    subset_generated <- subset_generated |>
      dplyr::mutate(prediction = replace(
        .data$prediction, .data$quantile == qi,
        values = rep(qi * 100,length(unique(subset$target_end_date)))
      ))
  }
  
  # Changing the true values at each time point (for each quantile the same)
  for (i in seq(1,length(unique(subset_generated$target_end_date))) ) {
    date <- unique(subset_generated$target_end_date)[i]
    value <- true_values_new[i]
    
    subset_generated <- subset_generated |>
      dplyr::mutate(true_value = replace(
        .data$true_value, .data$target_end_date == date,
        values = rep(value,length(unique(subset_generated$quantile)))
      ))
  }
  # adjustment based on x value e.g. line search
  wis_vec <- c()
  for (x in x_vec){
    
    subset_generated <- subset_generated |>
      dplyr::mutate(prediction = replace(
        .data$prediction, .data$quantile == qs[1],
        values = rep(qs[1] * 100 + (qs[1]-qs[2])*100 * (x-1),length(unique(subset$target_end_date)))
      ))
    subset_generated <- subset_generated |>
      dplyr::mutate(prediction = replace(
        .data$prediction, .data$quantile == qs[3],
        values = rep(qs[3] * 100 + (qs[3]-qs[2])*100 * (x-1),length(unique(subset$target_end_date)))
      ))
    
    res <- subset_generated |>
      scoringutils::score() |>
      scoringutils::summarise_scores(by = c("location"))

    wis_val <- res$interval_score
    wis_vec <- c(wis_vec,wis_val)
    
  }
  return(wis_vec)
}
  
```

# Testing QSA Uniform

We test the BFGS method in the WIS calculation and compare it to the results of a line search.

## Test 1: Using BFGS, if QSA covers the optimal number of observations but the intervals dont have the optimal size, does it adjust properly?

### Make intervals smaller?

- Expectation: QSA should make intervals smaller by 50%.
- Observation: Intervals become smaller by BFGs as well as line search.

```{r}
x <- seq(-1,3,0.01)
y <- wis_calc_pac(x,
              qs = c(0.3, 0.5, 0.7),
              true_values_new = c(rep(50,4), 60, rep(90,5)))

df_xy <- data.frame(x = x, y = y)

ggplot() +
  geom_line(data = df_xy, aes(x, y), colour = "black")
```

```{r}
test(qs = c(0.3, 0.5, 0.7),
     error_factor = 1,
     true_values_new = c(rep(50,4), 60, rep(90,5)),
     methods = "qsa_uniform",
     cv_init_training = NULL,
     quantile_plot = 0.3)
```

```{r}
test(qs = c(0.3, 0.5, 0.7),
     error_factor = 1,
     true_values_new = c(rep(50,4), 60, rep(90,5)),
     methods = "qsa_uniform",
     cv_init_training = NULL,
     quantile_plot = 0.3,
     optim_method = "line_search")
```

### Make intervals larger?

- Expectation: QSA should make intervals larger because observations outside the intervall are very far away
- Observation: intervals become larger in BFGS as well as line search. Both work.

```{r}
x <- seq(0,5,0.1)
y <- wis_calc_pac(x,
              qs = c(0.2, 0.5, 0.8),
              true_values_new = c(rep(50,4), 60, rep(100,5)))

df_xy <- data.frame(x = x, y = y)

ggplot() +
  geom_line(data = df_xy, aes(x, y), colour = "black")
```


```{r}
test(qs = c(0.2, 0.5, 0.8),
     error_factor = 1,
     true_values_new = c(rep(50,4), 60, rep(100,5)),
     methods = "qsa_uniform",
     cv_init_training = NULL,
     quantile_plot = 0.2)
```

```{r}
test(qs = c(0.2, 0.5, 0.8),
     error_factor = 1,
     true_values_new = c(rep(50,4), 60, rep(100,5)),
     methods = "qsa_uniform",
     cv_init_training = NULL,
     quantile_plot = 0.2,
     optim_method = "line_search")
```


## Test 2: Using BFGS, if QSA covers to many observations and intervals dont have the optimal size, does it adjust properly?

### Make intervals smaller?

Expectation: QSA should make intervals smaller
Observation: makes intervals smaller so that then encompase at least 5 values, which in this case means all values

```{r}
x <- seq(0,3,0.1)
y <- wis_calc_pac(x,
              qs = c(0.25, 0.5, 0.75),
              true_values_new = c(rep(50,4), 60, rep(40,5)))

df_xy <- data.frame(x = x, y = y)

ggplot() +
  geom_line(data = df_xy, aes(x, y), colour = "black")
```

```{r}
test(qs = c(0.25, 0.5, 0.75),
     error_factor = 1,
     true_values_new = c(rep(50,4), 60, rep(40,5)),
     methods = "qsa_uniform",
     cv_init_training = NULL,
     quantile_plot = 0.25)
```

```{r}
test(qs = c(0.25, 0.5, 0.75),
     error_factor = 1,
     true_values_new = c(rep(50,4), 60, rep(40,5)),
     methods = "qsa_uniform",
     cv_init_training = NULL,
     quantile_plot = 0.25,
     optim_method = "line_search")
```

### Make intervals larger?

Expectation: QSA should make intervals larger
Observation: makes intervals larger so that then encompase at least 5 values

```{r}
x <- seq(0,3,0.1)
y <- wis_calc_pac(x,
              qs = c(0.25, 0.5, 0.75),
              true_values_new = c(rep(50,4), 80, rep(5,5)))

df_xy <- data.frame(x = x, y = y)

ggplot() +
  geom_line(data = df_xy, aes(x, y), colour = "black")
```

```{r}
test(qs = c(0.25, 0.5, 0.75),
     error_factor = 1,
     true_values_new = c(rep(50,4), 80, rep(5,5)),
     methods = "qsa_uniform",
     cv_init_training = NULL,
     quantile_plot = 0.25)
```

```{r}
test(qs = c(0.25, 0.5, 0.75),
     error_factor = 1,
     true_values_new = c(rep(50,4), 80, rep(5,5)),
     methods = "qsa_uniform",
     cv_init_training = NULL,
     quantile_plot = 0.25,
     optim_method = "line_search")
```

## Conclusion

QSA with the BFGS method works as expected. We see that it does the same adjustment as we would expect with a line search seen by the line search plots and its method results from optimizing. However BFGS is faster than the line search, especially the larger one sets the line search parameter space.

# Testing QSA Uniform with different Optimizers

## Test 3: See if other optimizing methods, available in optim are able to adjust intervals correctly?

We thus repeat Test 1 with different optimizers.

### Make intervals smaller?

```{r}
x <- seq(0,3,0.1)
y <- wis_calc_pac(x,
              qs = c(0.3, 0.5, 0.7),
              true_values_new = c(rep(50,4), 60, rep(20,5)))

df_xy <- data.frame(x = x, y = y)

ggplot() +
  geom_line(data = df_xy, aes(x, y), colour = "black")
```
We look at the linesearch results as benchmark:

```{r}
test(qs = c(0.3, 0.5, 0.7),
     error_factor = 1,
     true_values_new = c(rep(50,4), 60, rep(20,5)),
     methods = "qsa_uniform",
     cv_init_training = NULL,
     quantile_plot = 0.3,
     optim_method = "line_search")
```
#### CG

Method: unkown
Observation: makes the correct adjustment.

```{r}
test(qs = c(0.3, 0.5, 0.7),
     error_factor = 1,
     true_values_new = c(rep(50,4), 60, rep(20,5)),
     methods = "qsa_uniform",
     cv_init_training = NULL,
     quantile_plot = 0.3,
     optim_method = "CG")
```

#### L-BFGS-B

Method: BFGS with bounds and limited memory.
Observation: Doesnt really work as it makes to small adjustments. But it is much faster than BFGS, probably due to limited memory.

```{r}
test(qs = c(0.3, 0.5, 0.7),
     error_factor = 1,
     true_values_new = c(rep(50,4), 60, rep(20,5)),
     methods = "qsa_uniform",
     cv_init_training = NULL,
     quantile_plot = 0.3,
     optim_method = "L-BFGS-B")
```

#### SANN

Method: unkown
Observation: Doesnt work, takes very long and destroys R session.

```{r}
#test(qs = c(0.25, 0.5, 0.75),
#     error_factor = 1,
#     true_values_new = c(rep(50,4), 60, rep(20,5)),
#     methods = "qsa_uniform",
#     cv_init_training = NULL,
#     quantile_plot = 0.25,
#     optim_method = "SANN")
```

#### Brent

Method: works by using the bisection and secant methods. The first looks to reduce the interval in which optimum can lie, thats why the method requires bounds. The second comes closer to the optimum e.g. root by drawing secant lines to approximate the function.

Observation: It works but does not get the exact solution as it should. It gives back  different results for different bounds which is due to its heuristic approach. The smaller bounds are the faster it is. 

```{r}
test(qs = c(0.3, 0.5, 0.7),
     error_factor = 1,
     true_values_new = c(rep(50,4), 60, rep(20,5)),
     methods = "qsa_uniform",
     cv_init_training = NULL,
     quantile_plot = 0.3,
     optim_method = "Brent",
     lower_bound_optim = -100,
     upper_bound_optim = 100)
```

```{r}
test(qs = c(0.3, 0.5, 0.7),
     error_factor = 1,
     true_values_new = c(rep(50,4), 60, rep(20,5)),
     methods = "qsa_uniform",
     cv_init_training = NULL,
     quantile_plot = 0.3,
     optim_method = "Brent",
     lower_bound_optim = -50,
     upper_bound_optim = 50)
```

```{r}
test(qs = c(0.3, 0.5, 0.7),
     error_factor = 1,
     true_values_new = c(rep(50,4), 60, rep(20,5)),
     methods = "qsa_uniform",
     cv_init_training = NULL,
     quantile_plot = 0.3,
     optim_method = "Brent",
     lower_bound_optim = 0,
     upper_bound_optim = 10)
```

```{r}
test(qs = c(0.3, 0.5, 0.7),
     error_factor = 1,
     true_values_new = c(rep(50,4), 60, rep(20,5)),
     methods = "qsa_uniform",
     cv_init_training = NULL,
     quantile_plot = 0.3,
     optim_method = "Brent",
     lower_bound_optim = 0,
     upper_bound_optim = 3)
```

## BFGS vs L-BFGS-B

### BFGS
```{r}
library(tictoc)

test <- function(qs = c(0.25, 0.5, 0.75),
                 error_factor = 1,
                 true_values_new = c(rep(50,4), 55, rep(5,5), rep(c(50,5),8),50),
                 methods = "qsa_uniform",
                 cv_init_training = NULL,
                 quantile_plot = 0.25,
                 optim_method = "BFGS",
                 lower_bound_optim = 0, 
                 upper_bound_optim = 5){
  
  # Get a subset of original data as template 
  # We only make it have the length of the true values (max remains 25)
  df <- read.csv(here::here("data_modified", "hub_data_2_incidences.csv"))
  m <- "epiforecasts-EpiExpert"
  l <- "GB"
  t <- "Cases"
  h <- 1
  subset <- dplyr::filter(df, model == m & location == l 
                          & target_type == t & horizon == h 
                          & quantile %in% qs 
                          & target_end_date %in% sort(unique(df$target_end_date))[0:length(true_values_new)])
  
  # Changing the quantile predictions to the value of the quantile * 100 * error_factor
  subset_generated <- subset
  for (qi in qs) {
    subset_generated <- subset_generated |>
      dplyr::mutate(prediction = replace(
        .data$prediction, .data$quantile == qi,
        values = rep(qi * 100 * error_factor,length(unique(subset$target_end_date)))
      ))
  }
  
  # Changing the true values at each time point (for each quantile the same)
  for (i in seq(0,length(unique(subset_generated$target_end_date))) ) {
    date <- unique(subset_generated$target_end_date)[i]
    value <- true_values_new[i]
    
    subset_generated <- subset_generated |>
      dplyr::mutate(true_value = replace(
        .data$true_value, .data$target_end_date == date,
        values = rep(value,length(unique(subset_generated$quantile)))
      ))
  }
  
  # Run QSA
  df_updated <- update_predictions(subset_generated, methods = methods,
                                   models = m, locations = l, target_types = t,
                                   horizons = h, quantiles = qs,
                                   cv_init_training = cv_init_training, 
                                   penalty_weight=NULL, 
                                   optim_method = optim_method, 
                                   lower_bound_optim = lower_bound_optim, 
                                   upper_bound_optim = upper_bound_optim,
                                   return_list = TRUE)
  
  # Combine DataFrames so that we can plot results
  df_combined <- df_updated |> collect_predictions()
  
  return(df_combined)
}

tic()

#   ____________________________________________________________________________
#   Test that no adjustment of intervals is done if in optimum              ####

#qsa_uniform
qs = c(0.25, 0.5, 0.75)
true_values_new = c(rep(50,4), 60, rep(20,5))

df_combined <- test(qs=qs, true_values_new=true_values_new, methods = "qsa_uniform")
original_prediction_sorted <- df_combined[0:30,]$prediction 
updated_prediction_sorted <- df_combined[31:60,][order(df_combined[31:60,]$forecast_date),]$prediction 

test_that("original and updated predictions are identical for qsa_uniform with optim method BFGS", {
  expect_equal(original_prediction_sorted, updated_prediction_sorted)
})

#qsa_flexible_symmetric
qs = c(0.25, 0.5, 0.75)
true_values_new = c(rep(50,4), 60, rep(20,5))

df_combined <- test(qs=qs, true_values_new=true_values_new, methods = "qsa_flexible_symmetric")
original_prediction_sorted <- df_combined[0:30,]$prediction 
updated_prediction_sorted <- df_combined[31:60,][order(df_combined[31:60,]$forecast_date),]$prediction 

test_that("original and updated predictions are identical for qsa_flexible_symmetric with optim method BFGS", {
  expect_equal(original_prediction_sorted, updated_prediction_sorted)
})

#qsa_flexible
qs = c(0.25, 0.5, 0.75)
true_values_new = c(rep(50,4), 0,0, 100,100)

df_combined <- test(qs=qs, true_values_new=true_values_new, methods = "qsa_flexible")
original_prediction_sorted <- df_combined[0:24,]$prediction 
updated_prediction_sorted <- df_combined[25:48,][order(df_combined[25:48,]$forecast_date),]$prediction 

test_that("original and updated predictions are identical for qsa_flexible with optim method BFGS", {
  expect_equal(original_prediction_sorted, updated_prediction_sorted)
})


#   ____________________________________________________________________________
#   Test that intervals are decreased till optimum                          ####

#qsa_uniform
qs = c(0.3, 0.5, 0.7)
true_values_new = c(rep(50,4), 60, rep(90,5))

df_combined <- test(qs=qs, true_values_new=true_values_new, methods = "qsa_uniform")

updated_prediction_sorted <- df_combined[31:60,][order(df_combined[31:60,]$forecast_date),]$prediction 
optimum <- rep(c(40, 50, 60),10)

test_that("updated predictions are rounded equal to expected optimum for qsa_uniform with optim method BFGS", {
  expect_equal(TRUE, any(abs(optimum - updated_prediction_sorted) < 1))
})

#qsa_flexible_symmetric
qs = c(0.3, 0.5, 0.7)
true_values_new = c(rep(50,4), 60, rep(90,5))

df_combined <- test(qs=qs, true_values_new=true_values_new, methods = "qsa_flexible_symmetric")

updated_prediction_sorted <- df_combined[31:60,][order(df_combined[31:60,]$forecast_date),]$prediction 
optimum <- rep(c(40, 50, 60),10)

test_that("original and updated predictions are identical for qsa_flexible_symmetric with optim method BFGS", {
  expect_equal(TRUE, any(abs(optimum - updated_prediction_sorted) < 1))
})

#qsa_flexible
qs = c(0.3, 0.5, 0.7)
true_values_new = c(rep(50,4), 60, rep(90,5))

df_combined <- test(qs=qs, true_values_new=true_values_new, methods = "qsa_flexible")

updated_prediction_sorted <- df_combined[31:60,][order(df_combined[31:60,]$forecast_date),]$prediction 
optimum <- rep(c(50, 50, 90),10)

test_that("original and updated predictions are identical for qsa_flexible with optim method BFGS", {
  expect_equal(TRUE, any(abs(optimum - updated_prediction_sorted) < 1))
})


#   ____________________________________________________________________________
#   Test that intervals are increased till optimum                          ####

#qsa_uniform
qs = c(0.2, 0.5, 0.8)
true_values_new = c(rep(50,4), 60, rep(90,5))

df_combined <- test(qs=qs, true_values_new=true_values_new, methods = "qsa_uniform")

updated_prediction_sorted <- df_combined[31:60,][order(df_combined[31:60,]$forecast_date),]$prediction 
optimum <- rep(c(10, 50, 90),10)

test_that("updated predictions are rounded equal to expected optimum for qsa_uniform with optim method BFGS", {
  expect_equal(TRUE, any(abs(optimum - updated_prediction_sorted) < 1))
})

#qsa_flexible_symmetric
qs = c(0.2, 0.5, 0.8)
true_values_new = c(rep(50,4), 60, rep(90,5))

df_combined <- test(qs=qs, true_values_new=true_values_new, methods = "qsa_flexible_symmetric")

updated_prediction_sorted <- df_combined[31:60,][order(df_combined[31:60,]$forecast_date),]$prediction 
optimum <- rep(c(10, 50, 90),10)

test_that("original and updated predictions are identical for qsa_flexible_symmetric with optim method BFGS", {
  expect_equal(TRUE, any(abs(optimum - updated_prediction_sorted) < 1))
})

#qsa_flexible
qs = c(0.2, 0.5, 0.8)
true_values_new = c(rep(50,4), 60, rep(90,5))

df_combined <- test(qs=qs, true_values_new=true_values_new, methods = "qsa_flexible")

updated_prediction_sorted <- df_combined[31:60,][order(df_combined[31:60,]$forecast_date),]$prediction 
optimum <- rep(c(50, 50, 90),10)

test_that("original and updated predictions are identical for qsa_flexible with optim method BFGS", {
  expect_equal(TRUE, any(abs(optimum - updated_prediction_sorted) < 1))
})


toc()
```
It Took 73.542 sec.

### L-BGS-B


```{r}
library(tictoc)

test <- function(qs = c(0.25, 0.5, 0.75),
                 error_factor = 1,
                 true_values_new = c(rep(50,4), 55, rep(5,5), rep(c(50,5),8),50),
                 methods = "qsa_uniform",
                 cv_init_training = NULL,
                 quantile_plot = 0.25,
                 optim_method = "L-BFGS-B",
                 lower_bound_optim = 0, 
                 upper_bound_optim = 5){
  
  # Get a subset of original data as template 
  # We only make it have the length of the true values (max remains 25)
  df <- read.csv(here::here("data_modified", "hub_data_2_incidences.csv"))
  m <- "epiforecasts-EpiExpert"
  l <- "GB"
  t <- "Cases"
  h <- 1
  subset <- dplyr::filter(df, model == m & location == l 
                          & target_type == t & horizon == h 
                          & quantile %in% qs 
                          & target_end_date %in% sort(unique(df$target_end_date))[0:length(true_values_new)])
  
  # Changing the quantile predictions to the value of the quantile * 100 * error_factor
  subset_generated <- subset
  for (qi in qs) {
    subset_generated <- subset_generated |>
      dplyr::mutate(prediction = replace(
        .data$prediction, .data$quantile == qi,
        values = rep(qi * 100 * error_factor,length(unique(subset$target_end_date)))
      ))
  }
  
  # Changing the true values at each time point (for each quantile the same)
  for (i in seq(0,length(unique(subset_generated$target_end_date))) ) {
    date <- unique(subset_generated$target_end_date)[i]
    value <- true_values_new[i]
    
    subset_generated <- subset_generated |>
      dplyr::mutate(true_value = replace(
        .data$true_value, .data$target_end_date == date,
        values = rep(value,length(unique(subset_generated$quantile)))
      ))
  }
  
  # Run QSA
  df_updated <- update_predictions(subset_generated, methods = methods,
                                   models = m, locations = l, target_types = t,
                                   horizons = h, quantiles = qs,
                                   cv_init_training = cv_init_training, 
                                   penalty_weight=NULL, 
                                   optim_method = optim_method, 
                                   lower_bound_optim = lower_bound_optim, 
                                   upper_bound_optim = upper_bound_optim,
                                   return_list = TRUE)
  
  # Combine DataFrames so that we can plot results
  df_combined <- df_updated |> collect_predictions()
  
  return(df_combined)
}

tic()

#   ____________________________________________________________________________
#   Test that no adjustment of intervals is done if in optimum              ####

#qsa_uniform
qs = c(0.25, 0.5, 0.75)
true_values_new = c(rep(50,4), 60, rep(20,5))

df_combined <- test(qs=qs, true_values_new=true_values_new, methods = "qsa_uniform")
original_prediction_sorted <- df_combined[0:30,]$prediction 
updated_prediction_sorted <- df_combined[31:60,][order(df_combined[31:60,]$forecast_date),]$prediction 

test_that("original and updated predictions are identical for qsa_uniform with optim method BFGS", {
  expect_equal(original_prediction_sorted, updated_prediction_sorted)
})

#qsa_flexible_symmetric
qs = c(0.25, 0.5, 0.75)
true_values_new = c(rep(50,4), 60, rep(20,5))

df_combined <- test(qs=qs, true_values_new=true_values_new, methods = "qsa_flexible_symmetric")
original_prediction_sorted <- df_combined[0:30,]$prediction 
updated_prediction_sorted <- df_combined[31:60,][order(df_combined[31:60,]$forecast_date),]$prediction 

test_that("original and updated predictions are identical for qsa_flexible_symmetric with optim method BFGS", {
  expect_equal(original_prediction_sorted, updated_prediction_sorted)
})

#qsa_flexible
qs = c(0.25, 0.5, 0.75)
true_values_new = c(rep(50,4), 0,0, 100,100)

df_combined <- test(qs=qs, true_values_new=true_values_new, methods = "qsa_flexible")
original_prediction_sorted <- df_combined[0:24,]$prediction 
updated_prediction_sorted <- df_combined[25:48,][order(df_combined[25:48,]$forecast_date),]$prediction 

test_that("original and updated predictions are identical for qsa_flexible with optim method BFGS", {
  expect_equal(original_prediction_sorted, updated_prediction_sorted)
})


#   ____________________________________________________________________________
#   Test that intervals are decreased till optimum                          ####

#qsa_uniform
qs = c(0.3, 0.5, 0.7)
true_values_new = c(rep(50,4), 60, rep(90,5))

df_combined <- test(qs=qs, true_values_new=true_values_new, methods = "qsa_uniform")

updated_prediction_sorted <- df_combined[31:60,][order(df_combined[31:60,]$forecast_date),]$prediction 
optimum <- rep(c(40, 50, 60),10)

test_that("updated predictions are rounded equal to expected optimum for qsa_uniform with optim method BFGS", {
  expect_equal(TRUE, any(abs(optimum - updated_prediction_sorted) < 1))
})

#qsa_flexible_symmetric
qs = c(0.3, 0.5, 0.7)
true_values_new = c(rep(50,4), 60, rep(90,5))

df_combined <- test(qs=qs, true_values_new=true_values_new, methods = "qsa_flexible_symmetric")

updated_prediction_sorted <- df_combined[31:60,][order(df_combined[31:60,]$forecast_date),]$prediction 
optimum <- rep(c(40, 50, 60),10)

test_that("original and updated predictions are identical for qsa_flexible_symmetric with optim method BFGS", {
  expect_equal(TRUE, any(abs(optimum - updated_prediction_sorted) < 1))
})

#qsa_flexible
qs = c(0.3, 0.5, 0.7)
true_values_new = c(rep(50,4), 60, rep(90,5))

df_combined <- test(qs=qs, true_values_new=true_values_new, methods = "qsa_flexible")

updated_prediction_sorted <- df_combined[31:60,][order(df_combined[31:60,]$forecast_date),]$prediction 
optimum <- rep(c(50, 50, 90),10)

test_that("original and updated predictions are identical for qsa_flexible with optim method BFGS", {
  expect_equal(TRUE, any(abs(optimum - updated_prediction_sorted) < 1))
})


#   ____________________________________________________________________________
#   Test that intervals are increased till optimum                          ####

#qsa_uniform
qs = c(0.2, 0.5, 0.8)
true_values_new = c(rep(50,4), 60, rep(90,5))

df_combined <- test(qs=qs, true_values_new=true_values_new, methods = "qsa_uniform")

updated_prediction_sorted <- df_combined[31:60,][order(df_combined[31:60,]$forecast_date),]$prediction 
optimum <- rep(c(10, 50, 90),10)

test_that("updated predictions are rounded equal to expected optimum for qsa_uniform with optim method BFGS", {
  expect_equal(TRUE, any(abs(optimum - updated_prediction_sorted) < 1))
})

#qsa_flexible_symmetric
qs = c(0.2, 0.5, 0.8)
true_values_new = c(rep(50,4), 60, rep(90,5))

df_combined <- test(qs=qs, true_values_new=true_values_new, methods = "qsa_flexible_symmetric")

updated_prediction_sorted <- df_combined[31:60,][order(df_combined[31:60,]$forecast_date),]$prediction 
optimum <- rep(c(10, 50, 90),10)

test_that("original and updated predictions are identical for qsa_flexible_symmetric with optim method BFGS", {
  expect_equal(TRUE, any(abs(optimum - updated_prediction_sorted) < 1))
})

#qsa_flexible
qs = c(0.2, 0.5, 0.8)
true_values_new = c(rep(50,4), 60, rep(90,5))

df_combined <- test(qs=qs, true_values_new=true_values_new, methods = "qsa_flexible")

updated_prediction_sorted <- df_combined[31:60,][order(df_combined[31:60,]$forecast_date),]$prediction 
optimum <- rep(c(50, 50, 90),10)

test_that("original and updated predictions are identical for qsa_flexible with optim method BFGS", {
  expect_equal(TRUE, any(abs(optimum - updated_prediction_sorted) < 1))
})

toc()
```

It Took 59.087 sec.


## Conclusion

We can use the BFGS method without any issues. however default is the limited version L-BFGS-B as it is faster.
