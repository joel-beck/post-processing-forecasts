---
title: "CQR Results for UK Data and European Forecast Hub"
author: "Joel Beck"
output:
  bookdown::html_document2:
    theme: flatly
    highlight: pygments  # kate
    toc: TRUE
    toc_float: TRUE
    toc_depth: 2
    number_sections: FALSE
    df_print: tibble
  bookdown::pdf_document2:
    highlight: tango  # pygments
    toc: FALSE
    number_sections: FALSE
    df_print: tibble
    latex_engine: pdflatex
    keep_tex: FALSE
urlcolor: blue
---

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE, fig.align = "center",
  out.width = "100%", dpi = 300, fig.asp = 0.618, collapse = TRUE
)
```

```{r libraries}
devtools::load_all(".")
library(patchwork)
```

# UK Data

```{r}
uk_cqr <- readr::read_rds(here::here("data_results", "uk_cqr.rds"))

uk_cqr |> dplyr::count(model)
```

The UK Dataset contains predictions from six different models.
Each of these models is connected to exactly two values for every combination of `target_type`, `horizon`, `quantile` and `target_end_date`(i.e. $2 \cdot 4 \cdot 23 \cdot 13 = 2392$ combinations): 
One value with the original prediction and one value for the adjusted prediction after applying the CQR method.

## Visualizing the raw data

First, one might be interested in any particular covariate combination.
The corresponding prediction interval before and after CQR can be plotted with the `plot_intervals()` function:

```{r}
plot_intervals(
  uk_cqr, model = "seabbs", target_type = "Cases", horizon = 3, quantile = 0.1
)
```

To visualize the trend along the horizon dimension or for different quantiles for each `target_type` separately, we can use the `plot_intervals_grid()` function:

```{r}
plot_intervals_grid(
  uk_cqr, model = "seabbs", quantile = 0.1, facet_by = "horizon"
)
```

The plots reveal two main findings:

- CQR seems to make the prediction intervals larger.
Since the `seabbs` forecasts are contributed by a single human, this finding confirms the hypothesis that humans tend to be too confident in their own forecasts leading to narrow prediction intervals.

- A larger forecast horizon strongly correlates with higher uncertainty and, thus, wider prediction intervals.

According to this impression CQR produces adjusted, often larger, interval forecasts.
However, up to now we can **not** make a statement if the post-processed predictions are actually 'better'.

## Weighted Interval Score and the `scoringutils` package 

We evaluate the *quality* of forecast intervals with the *Weighted Interval Score*.
This metric is implemented by the `scoringutils` package.
It provides very fine control about the granularity of evaluation.

Let us first replicate the setting from the first plot and analyze if CQR actually improved the forecasts.
Since we are primarily interested in out-of-sample performance we filter the data frame down to the validation set:

```{r}
uk_cqr |> 
  extract_validation_set() |>
  scoringutils::score() |> 
  scoringutils::summarise_scores(by = c("method", "model", "target_type", "horizon", "quantile")) |>
  dplyr::filter(model == "seabbs", target_type == "Cases", horizon == 3, quantile == 0.1) |> 
  dplyr::select(method:dispersion)
```

Indeed, CQR leads to a lower interval score by increasing the dispersion/spread resulting in larger intervals compared to the original forecasts.

In contrast, we can aggregate over all models, target types, horizons and quantiles and evaluate the *overall* performance of the CQR method:

```{r}
uk_cqr |> 
  extract_validation_set() |>
  scoringutils::score() |> 
  scoringutils::summarise_scores(by = c("method")) |>
  dplyr::select(method:dispersion)
```

The result shows the same trend: better and wider overall prediction intervals.

Of particular interest might be the question which models benefitted most from CQR adjustments.
This question can, of course, be answered in a similar way to above by including `model` in the `by` argument of `summarise_scores()`.

## `eval_methods()` and `plot_eval()`  

In addition to the *absolute* interval score change, which can lose meaning in case of very different scales for each model, the *relative* or *percentage* change allows for direct comparisons on a level playing field.

The `eval_methods()` function displays the relative change of CQR compared to the original predictions.
Negative values indicate a score improvement, positive values on the other hand a larger and therefore worse interval score.
Note that the function automatically detects included post-processing methods from the input data frame and thus the string `'method'` does not have to be provided.
Moreover, all displayed values are **computed exclusively from the validation data**. 

```{r}
eval_methods(uk_cqr, summarise_by = "model")
```

The output reveals that CQR lead to improved performance for all models except `EuroCOVIDhub-baseline`.
For many categories the magnitudes of the relative change in the output table might still be difficult to compare, even when sorted in increasing order.
Thus, the table can be visualized either by bars (only available for a single specified category such as `model` in this case) or by a heatmap (the default):

```{r}
df_eval <- eval_methods(uk_cqr, summarise_by = "model")

p1 <- plot_eval(df_eval, heatmap = FALSE, base_size = 8) + ggplot2::labs(y = NULL)
p2 <- plot_eval(df_eval, base_size = 8) + ggplot2::labs(y = NULL)

p1 + p2
```
These two functions are not restricted to a single category (or even a single post-processing method).
To illustrate the multi-category case let's plot the relative improvements of CQR for each commbination of `model` and `quantile` and `model` and `horizon`, respectively:

```{r}
df_mod_quant <- eval_methods(uk_cqr, summarise_by = c("model", "quantile"))
p1 <- plot_eval(df_mod_quant, base_size = 8) + ggplot2::labs(x = NULL)

df_mod_hor <- eval_methods(uk_cqr, summarise_by = c("model", "horizon"))
p2 <- plot_eval(df_mod_hor, base_size = 8) + ggplot2::labs(y = NULL)

p1 + p2
```

Interestingly, for the `EuroCOVIDhub-baseline` model predicting constant values, CQR leads to worse interval scores for quantiles in the middle range as well as for extreme horizons but not vice versa!
Overall, however, the trends are similar to our conclusions before: 
Extreme quantiles and large horizons benefit most. 

It is worth noting that, starting from the plot output, you can always revert to the exact numerical values simply by analyzing the input data frame directly:

```{r}
df_mod_hor
```

## Further arguments to `eval_methods()` 

In case **all** of the values in a particular row or column of the `eval_methods()` output have the same sign, it might be useful to group the values in e.g. below and above average improvements.
One way to average ratios (here the difference in scores divided by the original score) is to take the geometric mean.
This can be achieved for rows and/or columns with the `row_averages` and `col_averages` arguments to `eval_methods()`:  

```{r}
df_mod_tar <- eval_methods(
  uk_cqr, summarise_by = c("model", "target_type"), 
  row_averages = TRUE, col_averages = TRUE
)
df_mod_tar
```

The last row computes geomtric means for each column and the last column computes geometric means for each row.
Since `target_type` only has two possible values, the geometric mean in the last column always lies strictly in between the two values of the corresponding row.

This table can be visualized in the same way as before:

```{r}
plot_eval(df_mod_tar)
```

The plot shows that CQR improvements are sometimes stronger for Cases and sometimes for Deaths depending on the model.

Finally, one might analyze both the performance for a combination of two categories as well as the marginal performance for a single category.
This can be done separately:

```{r, eval=FALSE}
eval_methods(uk_cqr, summarise_by = "model")
eval_methods(uk_cqr, summarise_by = "horizon")
eval_methods(uk_cqr, summarise_by = c("model", "horizon"))
```

However, jumping around between three different tables can be annoying.
For that reason the marginal changes can be added to the two-dimensional table with the `margins` argument:

```{r}
eval_mod_hor <- eval_methods(
  uk_cqr, summarise_by = c("model", "horizon"), margins = TRUE
)
```

Due to the large number of entries a picture is again convenient:

```{r}
plot_eval(eval_mod_hor)
```

While the marginal performance of all models except `EuroCOVIDhub-baseline` is improved by applying CQR, the *interaction* between those models and low horizons do not benefit from post-processing.



# European Forecast Hub

```{r}
hub_1 <- readr::read_rds(here::here("data_results", "hub_cqr_1.rds"))
hub_2 <- readr::read_rds(here::here("data_results", "hub_cqr_2.rds"))
hub_cqr <- dplyr::bind_rows(hub_1, hub_2)
```