---
title: "CQR Results for European Forecast Hub Data"
output:
  bookdown::html_document2:
    theme: flatly
    highlight: pygments
    toc: TRUE
    toc_float: TRUE
    number_sections: FALSE
    df_print: paged
  bookdown::pdf_document2:
    highlight: tango  
    toc: FALSE
    number_sections: FALSE
    df_print: tibble
    latex_engine: pdflatex
    keep_tex: FALSE
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE, fig.align = "center",
  out.width = "100%", dpi = 300, fig.asp = 0.618, collapse = TRUE
)
```

```{r libraries}
pacman::p_load(dplyr, scoringutils)
devtools::load_all()
```

```{r}
hub_data_1 <- readr::read_csv(here::here("data", "full-data-european-forecast-hub-1.csv"))
hub_data_2 <- readr::read_csv(here::here("data", "full-data-european-forecast-hub-2.csv"))

hub_data <- bind_rows(hub_data_1, hub_data_2)
```

```{r}
# number of data points
hub_data$target_end_date |> n_distinct()
```

```{r}
# column names of the dataset
colnames(hub_data)
```

```{r}
# unique models in our dataset
unique(hub_data$model)
```
# Analysis Approach

First we update all predictions with the cqr method. Then we will take a look at the results by subsequently aggregating the results over all columns except one. That way we hope to see whether a column has an infleunce on the preformance of the cqr method. In doing so we will both examine the training and validation set preformances. For simplicity reasons we fix the initial training period required for the crossvalidation at 10 initial obserevations.

```{r}
cv_init_training <- 10

xfull_results <- update_predictions(
  hub_data,
  methods = "cqr",
  models = setdiff(unique(hub_data$model),c("epiforecasts-EpiExpert")),
  cv_init_training = cv_init_training
) |>
  collect_predictions()
```

```{r}
# apparently there are no na values in the dataframe
sum(is.na(hub_data))
sum(is.null(hub_data))
```


```{r}
# no model has NA in its observations
for (m in unique(hub_data$model)){
  na_sum = sum(is.na(hub_data$prediction[hub_data$model == m]))
  print(cat(m,": ",na_sum))
}
```

```{r}
# Each model provides 32 Days of data
for (m in unique(hub_data$model)){
  na_sum = length(unique(hub_data$target_end_date))
  print(cat(m,": ",na_sum))
}
```

# Finding all imcomplete Series

In order for the weighted interval scores to be comparable we need to ensure that all series have the same length.
We found that the maximum length of a series in our Data was 32. In the following code we list all combinations of series defining parameter e.g. model, location, target_type that have incomplete data. Note that we assume that if data is completely available for one quantile and one horizon then it is available for all quantiles and horizons.


```{r}
# TODO: Find all combinations of series parameters where there are less than 10 observations

df <- hub_data
models = NULL #setdiff(unique(hub_data$model),c("epiforecasts-EpiExpert"))
locations = NULL #setdiff(unique(hub_data$location),c("GB","AT", "BE", "BG", "CH", "CY", "CZ"))
target_types = NULL
horizons = NULL
quantiles = NULL

# Preprocessing the df and inputs
preprocessed_list <- preprocess_df(
  df, models, locations, target_types, horizons, quantiles
  )

df_preprocessed <- preprocessed_list$df
models <- preprocessed_list$models
locations <- preprocessed_list$locations
target_types <- preprocessed_list$target_types
horizons <- preprocessed_list$horizons
quantiles <- preprocessed_list$quantiles

target_types <- target_types[1]
horizons <- horizons[1]
quantiles <- quantiles[1]

incomplete_series_table <- tibble(
    model = character(),
    location = character(),
    target_type = character(),
    horizon = numeric(),
    quantile = numeric()
)
row_num <- 1

for (model in models) {
  for (location in locations) {
    for (target_type in target_types) {
      for (horizon in horizons) {
        for (quantile in quantiles) {
          quantiles_list <- filter_combination(df, model, location, target_type, horizon, quantile)
          true_values <- quantiles_list$true_values
          if(length(true_values) < 32){
            incomplete_series_table[row_num,] <- list(model, location, target_type, horizon, quantile)
            row_num <- row_num + 1
            
            }
        }
      }
    }
  }
}
```

```{r}
paste0("Number of models where at least one series is incomplete: ",nrow(unique(incomplete_series_table["model"])))
paste0("Number of models in the dataset: ", nrow(unique(hub_data["model"])))

paste0("Number of locations where at least one series is incomplete: ", nrow(unique(incomplete_series_table["location"])))
paste0("Number of locations in the dataset: ", nrow(unique(hub_data["location"])))

paste0("Number of target types where at least one series is incomplete: ",nrow(unique(incomplete_series_table["target_type"])))
```

We find that we have at least one series missing for each of the countries in the dataset

```{r}
for (m in unique(incomplete_series_table["model"])[[1]]){
  print(m)
  print(nrow(incomplete_series_table %>% 
          filter(model == m, target_type == "Cases")) )
}
```
For Deaths however there are no series missing so apparently looking at cases is the issue?

```{r}
for (m in unique(incomplete_series_table["model"])[[1]]){
  print(m)
  print(nrow(incomplete_series_table %>% 
          filter(model == m, target_type == "Cases")) )
}
```




## Fit Models and analyze Performance on Training and Validation Set separately

```{r}
full_results <- update_predictions(
  hub_data,
  methods = "cqr", models, location, horizon = horizon,
  cv_init_training = cv_init_training
) |>
  collect_predictions()

training_results <- full_results |>
  extract_training_set() |>
  eval_forecasts(summarise_by = c("method", "model", "target_type"))

validation_results <- full_results |>
  extract_validation_set() |>
  eval_forecasts(summarise_by = c("method", "model", "target_type"))
```

```{r}
# cqr improves interval score for every model in the training set
training_results |>
  select(method:interval_score) |>
  arrange(interval_score)
```

```{r}
# mixed results on the validation set
validation_results |>
  select(method:interval_score) |>
  arrange(interval_score)
```

## Visualize CQR - Adjustments for Covid-19 Cases and Deaths

```{r}
model <- "epiforecasts-EpiExpert"

plot_intervals(
  full_results, model, location,
  target_type = "Cases", quantile = 0.05, horizon = horizon
)
```

```{r}
plot_intervals(
  full_results, model, location,
  target_type = "Deaths", quantile = 0.05, horizon = horizon
)
```
