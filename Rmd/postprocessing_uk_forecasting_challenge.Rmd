---
title: "postprocessing_uk_forecasting_challenge"
author: "Matthias Herp"
date: "11/30/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Post Processing UK Forecasting Challenge

This document presents results of post processing forecasting data from the UK Forecasting Challenge. 

#### Loading Packages and Data

We have the following variable columns in the data determining a forecasting series:
- models: all different forecasters
- target_type: Cases or Deaths
- horizons: 1,2,3,4 weeks ahead
- quantiles: 11 quantile pairs, omitting the median estimate

```{r}
# temporary and not recommended way, library(postforecasts) imports only functions with @export tag
# => requires more complete documentation
devtools::load_all(".")
library(scoringutils)
library(dplyr)

df <- read.csv(here::here("data", "full-data-uk-challenge.csv"))
```

## CQR: Conformalized Quantile Regression

The following code applies the CQR to the UK-Forecasting-Challenge data. 

#### Updating Forecasts

Our first step is to update forecasts for one model, the ensembled model "epiforecasts-EpiExpert", regading the target type "Cases", the horizon 1 and subsequently compare prediction interval metrics before and after applying CQR. For the update we loop over all quantile pairs as well as target types. We define a prediction interval pair by its lower boundary quantile. We finally combine the updated DataFrame with the preprocessed original Dataframe in order to prepare it for the following analysis.

```{r}
# Defining the Model
model <- unique(df$model)[2]

# Updating Predictions
df_updated <- update_predictions(df = df, method = "cqr", models = model, locations = "GB",
                                 horizons = 1)#, cv_init_training = 5)

# Preprocessing the original DF
preprocessed <- preprocess_df(df = df, models = model, locations = "GB",
                                 horizons = 1)
df_preprocessed <- preprocessed$df

# Combining the updated and original DFs
df_combined <- collect_predictions(original = df_preprocessed, cqr = df_updated)
```

#### Plotting one CI before and after updating for Cases

```{r}
plot_intervals(df = df_combined, model = model, target_type = "Cases",
                           quantile = 0.05, horizon = 1)
```

#### Evaluating Forecasting Updates

We evaluate the changes in the prediction intervals by using the "scoringutils" package. The main metric we look at is the interval_score. It is defined as: sharpness + underprediction + overprediction. We can see a slight improvement as the score drops.

```{r}
scores_combined <- df_combined |>
  filter(model == !!model) |>
  eval_forecasts(summarise_by = c("method", "model", "target_type"))

scores_combined |> 
  arrange(target_type, desc(method))
```


#### Updating Forecasts using Cross Validation

We again update our forecast but this time we use crossvalidation with a training set length of 5.

```{r}
# Defining the Model
model <- unique(df$model)[2]

# Updating Predictions
df_updated <- update_predictions(df = df, method = "cqr", models = model, locations = "GB",
                                 horizons = 1, cv_init_training = 5)

# Preprocessing the original DF
preprocessed <- preprocess_df(df = df, models = model, locations = "GB",
                                 horizons = 1)
df_preprocessed <- preprocessed$df

# Combining the updated and original DFs
df_combined <- collect_predictions(original = df_preprocessed, cqr = df_updated)
```


#### Plotting one CI before and after updating for Cases

We can clearly see that know all intervals are not updated by the same margin. Especially note how the intervals strongly increase after the last week of july where the observed cases are below the interval. This observed error leads CQR to increase the prediction intervals in the following time points.

```{r}
plot_intervals(df = df_combined, model = model, target_type = "Cases",
                           quantile = 0.05, horizon = 1)
```

#### Evaluating Forecasting Updates

We can see a slight worsening as the score increases. This is most likely due to the increasing of the intervals after the overestimation at the ned of july. Furthermore this might also indicate a too small training sample, which seems reasonable as 1 observation outside the intervals, in a 5 observations training set, would lead all intervals larger than 80% to increase automatically.

```{r}
scores_combined <- df_combined |>
  filter(model == !!model) |>
  eval_forecasts(summarise_by = c("method", "model", "target_type"))

scores_combined |> 
  arrange(target_type, desc(method))
```