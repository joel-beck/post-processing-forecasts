---
title: "postprocessing_uk_forecasting_challenge"
author: "Matthias Herp"
date: "11/30/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Post Processing UK Forecasting Challenge

This document presents results of post processing forecasting data from the UK Forecasting Challenge. 

#### Loading Packages and Data

We have the following variable columns in the data determining a forecasting series:
- models: all different forecasters
- target_type: Cases or Deaths
- horizons: 1,2,3,4 weeks ahead
- quantiles: 11 quantile pairs, omitting the median estimate

```{r}
# temporary and not recommended way, library(postforecasts) imports only functions with @export tag
# => requires more complete documentation
devtools::load_all(".")
library(scoringutils)
library(dplyr)

df <- read.csv(here::here("data", "full-data-uk-challenge.csv"))
```

## CQR: Conformalized Quantile Regression

The following code applies the cqr to the data. The helper functions "plot_cqr_results" and ""

#### Exploring Results

Our first step is to apply CQR to an example model and provide plots in order to illustrate the updates to the upper and lower boundaries. Note that below, we keep the model, target type and horizon constant and simply change the forecasted quantile. For our illustrated model we choose the ensembled model "epiforecasts-EpiExpert".

```{r}
model <- unique(df$model)[2]
horizon <- 1
```

#### Updating Forecasts

As a next step we update all forecasts for one model, the ensembled model "epiforecasts-EpiExpert", regading the target type "Cases" and subsequently compare prediction intervall metrics before and after applying CQR. For the update we loop over all forecasting horizons as well as all quantile pairs. we define a prediction interval pair by its lower boundary quantile. Finally we run a short test at the end which returns us the intitial as well as the updated values by comparing the two dataframes. If the diffrence between the predictions is constant over all observations and changes sign between the upper and lower quantiles this indecates that CQR was applied adequatly.

```{r}
df_updated <- update_predictions(df = df, method = "cqr", models = model, locations = "GB",
                                 horizons = 1, cv_init_training = 5)

df_combined <- collect_predictions(original = df, cqr = df_updated)

#test_df_updated(
#  df = df, df_updated = df_updated, example_model = model,
#  t = "Cases", h = 1, q = 0.025
#)
```

```{r}
plot_intervals(df = df_combined, model = model, target_type = "Cases",
                           quantile = 0.05, horizon = 1)
```

#### Evaluating Forecasting Updates

We evaluate the changes in the prediction intervals by using the "scoringutils" package. The main metric we look at is the interval_score. It is defined as: sharpness + underprediction + overprediction. We can see a clear inprovement as the score drops.

```{r}
scores_combined <- df_combined |>
  filter(model == !!model) |>
  eval_forecasts(summarise_by = c("method", "model", "target_type"))

scores_combined |> 
  arrange(target_type, desc(method))

#scores_original <- eval_forecasts(data = df, summarise_by = c("model", "target_type"))
#scores_updated <- eval_forecasts(data = df_updated, summarise_by = c("model", "target_type"))

#scores_original[model == model & target_type == "Cases"]
#scores_updated[model == model & target_type == "Cases"]
```
