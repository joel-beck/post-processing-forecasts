---
title: "postprocessing_uk_forecasting_challenge"
author: "Matthias Herp"
date: "11/30/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Post Processing UK Forecasting Challenge

This document presents results of post processing forecasting data from the UK Forecasting Challenge. 

#### Loading Packages and Data

We have the following variable columns in the data determining a forecasting series:
- models: all different forecasters
- target_type: Cases or Deaths
- horizons: 1,2,3,4 weeks ahead
- quantiles: 11 quantile pairs, omitting the median estimate

```{r}
# temporary and not recommended way, library(postforecasts) imports only functions with @export tag
# => requires more complete documentation
devtools::load_all(".")
library(scoringutils)
library(dplyr)

df <- read.csv(here::here("data", "full-data-uk-challenge.csv"))
```

## CQR: Conformalized Quantile Regression

The following code applies the CQR to the UK-Forecasting-Challenge data.

#### Updating Forecasts

Our first step is to update forecasted (symmetric) prediction intervals using the postprocessing package pipeline. For now, restrict ourselves to analyzing the results of the ensembled model named "epiforecasts-EpiExpert". We do however update its results for both target types, the 4 forecasting horizons as well as all (symmetric) quantile pairs. 

```{r}
df_combined <- update_predictions(df,
  methods = "cqr",
  model = "epiforecasts-EpiExpert",
  location = "GB",
  return_list = TRUE
) |>
  collect_predictions()
```

```{r}
attr(df_combined, "cv_init_training")
```


#### Plotting one CI before and after updating for Cases

```{r}
plot_intervals(
  df = df_combined, model = "epiforecasts-EpiExpert", target_type = "Cases",
  quantile = 0.05, horizon = 1
)
```

#### Evaluating Forecasting Updates

We evaluate the changes in the prediction intervals by using the "scoringutils" package. The main metric we look at is the interval_score. It is defined as: sharpness + underprediction + overprediction. We can see a slight improvement as the score drops.

```{r}
eval_forecasts(df_combined, summarise_by = c("method", "model", "target_type")) |>
  arrange(target_type, desc(method))
```


#### Updating Forecasts using Cross Validation

We again update our forecast, however this time we use crossvalidation with a training set length of 5.

```{r}
df_combined <- update_predictions(df,
  methods = "cqr",
  model = "epiforecasts-EpiExpert",
  location = "GB",
  cv_init_training = 5,
  return_list = TRUE
) |>
  collect_predictions()
```


#### Plotting one CI before and after updating for Cases

We can clearly see that now all intervals are not updated by the same margin. Especially note how the intervals strongly increase after the last week of july where the observed cases are below the interval. This observed error leads CQR to increase the prediction intervals in the following time points.

```{r}
plot_intervals(
  df = df_combined, model = "epiforecasts-EpiExpert", target_type = "Cases",
  quantile = 0.05, horizon = 1
)
```

#### Evaluating Forecasting Updates

We can see that the forecasting intervals still improve in the validation set, allthough the improvement is smaller than in the prior case where CQR was trained on the full dataset.

```{r}
extract_validation_set(df_combined) |>
  eval_forecasts(summarise_by = c("method", "model", "target_type")) |>
  arrange(target_type, desc(method))
```
```{r}
extract_training_set(df_combined) |>
  eval_forecasts(summarise_by = c("method", "model", "target_type")) |>
  arrange(target_type, desc(method))
```

#### Examining Forecast update effect along the different parameters

The results indicate that CQR works better for longer forecasting horizons as well as larger confidence intervals. For short horizons of 1 and 2 weeks it has a slightly undesired effect. For longer Periods of 3 and 4 weeks it has a desired effect of reducing interval score. For quantiles we see that CQR improves larger Prediction intervalls much stronger as smaller ones.
The plot exemplifies how uncertainty is strongly underestimated for larger quantiles and higher horizons. This seems to be where CQR provides a benefit, even for a small dataset.

```{r}
extract_validation_set(df_combined) |>
  eval_forecasts(summarise_by = c("method", "model", "target_type", "horizon")) |>
  arrange(target_type, desc(horizon))
```

```{r}
extract_validation_set(df_combined) |>
  eval_forecasts(summarise_by = c("method", "model", "target_type", "quantile")) |>
  arrange(target_type, desc(quantile))
```

```{r}
plot_intervals(
  df = df_combined, model = "epiforecasts-EpiExpert", target_type = "Cases",
  quantile = 0.05, horizon = 4
)
```
