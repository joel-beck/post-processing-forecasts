---
title: "postprocessing_qsa_method"
author: "Matthias Herp"
date: "24/01/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Post Processing package with quantile spread adjustment

This document presents results of post processing forecasting using the quantile spread adjustment on the data from the UK Forecasting Challenge. its main purpose is for testing, e.g. proof of concept.

#### Loading Packages and Data

We have the following variable columns in the data determining a forecasting series:
- models: all different forecasters
- target_type: Cases or Deaths
- horizons: 1,2,3,4 weeks ahead
- quantiles: 11 quantile pairs, omitting the median estimate

```{r}
# temporary and not recommended way, library(postforecasts) imports only functions with @export tag
# => requires more complete documentation
devtools::load_all(".")
library(scoringutils)
library(dplyr)

######## Run the qspread_package script so that the functions get into the environment

df <- read.csv(here::here("data", "full-data-uk-challenge.csv"))
```



```{r}
#Defining the specific series to forecast and getting it from the total data
m <- "epiforecasts-EpiExpert"
l <- "GB"
t <- "Cases"
h <- 1
#cv_init_training <- 10
```

# QSA

QSA works without cross validation.

```{r}
update_predictions(df, methods = "qsa_uniform",
                   models = m, locations = l, target_types = t,
                   horizons = h, quantiles = NULL,
                   cv_init_training = NULL, penalty_weight=NULL, return_list = TRUE)
```

```{r}
update_predictions(df, methods = "qsa_uniform",
                   models = m, locations = l, target_types = t,
                   horizons = h, quantiles = NULL,
                   cv_init_training = 10, penalty_weight=NULL, return_list = TRUE)
```


```{r}
update_predictions(df, methods = "qsa_flexibel",
                   models = m, locations = l, target_types = t,
                   horizons = h, quantiles = NULL,
                   cv_init_training = NULL, return_list = TRUE)
```

```{r}
update_predictions(df, methods = "qsa_flexibel_symmetric",
                   models = m, locations = l, target_types = t,
                   horizons = h, quantiles = NULL,
                   cv_init_training = NULL, return_list = TRUE)
```

All three QSA flavors work. Note however that the flexible methods takes quite some time to optimize. The symmetric is much faster and the unique is quite fast. We tested for one series so if we do it for multiple ones the computational burden might be quite high for flexible and symmetric flavors.




```{r}
update_predictions(df, methods = "qsa_flexibel_symmetric",
                   models = m, locations = l, target_types = t,
                   horizons = h, quantiles = NULL,
                   cv_init_training = NULL, penalty_weight=10,return_list = TRUE)
```


```{r}
update_predictions(df, methods = "qsa_uniform",
                   models = m, locations = l, target_types = t,
                   horizons = h, quantiles = NULL,
                   cv_init_training = 10, return_list = TRUE)
```


```{r}
training_length <- 10
target_end_date_subset <-  sort(unique(df$target_end_date))[0:training_length]
# gets subset for training length
s <- dplyr::filter(df, model == m & location == l & target_type == t & horizon == h)

target_end_date_subset <-  sort(unique(s$target_end_date))[0:training_length]

s <- dplyr::filter(s, target_end_date %in% target_end_date_subset)

print(sort(unique(s$target_end_date)))
print(target_end_date_subset)

bind_rows(setNames(target_end_date_subset))[0, ]
t <- as_tibble(sapply(target_end_date_subset, function(x) character()),)
t      
```




# CQR

CQR still works.

```{r}
update_predictions(df, methods = "cqr",
                   models = m, locations = l, target_types = t,
                   horizons = h, quantiles = NULL,
                   cv_init_training = NULL, return_list = TRUE)
```



```{r}

```



