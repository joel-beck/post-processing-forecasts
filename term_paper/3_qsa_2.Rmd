---
output:
  bookdown::pdf_document2:
    includes:
      in_header: preamble.tex
    toc: FALSE
    highlight: tango
    number_sections: TRUE
    df_print: tibble
    latex_engine: pdflatex
bibliography: [paper.bib, packages.bib]
biblio-style: apalike
urlcolor: black
linkcolor: blue
links-as-notes: true
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE, fig.align = "center",
  out.width = "100%", dpi = 300, fig.asp = 0.618, collapse = TRUE
)

devtools::load_all(".")
library(dplyr)
library(ggplot2)
library(patchwork)

uk_qsa_uniform <- readr::read_rds(here::here("data_results", "uk_qsa_uniform.rds"))
uk_qsa_flexible_symmetric <- readr::read_rds(here::here("data_results", "uk_qsa_flexible_symmetric.rds"))
uk_qsa_flexible <- readr::read_rds(here::here("data_results", "uk_qsa_flexible.rds"))

uk_qsa_flexible_symmetric_no_original <- uk_qsa_flexible_symmetric |>
  filter(method=="qsa_flexible_symmetric")
uk_qsa_flexible_no_original <- uk_qsa_flexible |>
  filter(method=="qsa_flexible")

uk_qsa <- bind_rows(uk_qsa_uniform, uk_qsa_flexible_symmetric_no_original, uk_qsa_flexible_no_original)


#TODO: add empty spaces, less block text.
```

```{r, include=FALSE}
display_table <- function(df, caption, bold_header = TRUE, striped = FALSE) {
  tab <- df |>
    kableExtra::kbl(
      digits = 2, align = "c", booktabs = TRUE, caption = caption
    ) |>
    kableExtra::row_spec(row = 0, bold = bold_header) |>
    kableExtra::kable_styling(position = "center", full_width = FALSE)

  if (striped) {
    tab <- tab |> kableExtra::kable_styling(latex_options = "striped")
  }

  return(tab)
}
```


## Results

As for the CQR method, we investigate how well QSA performs for post-processing Covid-19 forecasts. We focus on the UK Covid-19 Forecasting Challenge data set due to computational restrictions.

```{r, qsa-table-metrics, echo=FALSE}
# Defining Table Metrics Values for all three Methods to use in the Text


# QSA Uniform
tab_uniform <- uk_qsa_uniform |>
  extract_validation_set() |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = c("method")) |>
  dplyr::select(method:dispersion, underprediction, overprediction)
  
uniform_wis_rel <- (tab_uniform$interval_score[2] - tab_uniform$interval_score[1]) / tab_uniform$interval_score[1] * 100

uniform_dis_fac <- tab_uniform$dispersion[2] / tab_uniform$dispersion[1]

uniform_under_pred_rel <- (tab_uniform$underprediction[2] - tab_uniform$underprediction[1]) / tab_uniform$underprediction[1] * 100

uniform_under_pred_val <- tab_uniform$underprediction[2] - tab_uniform$underprediction[1]

uniform_over_pred_rel <- (tab_uniform$overprediction[2] - tab_uniform$overprediction[1]) / tab_uniform$overprediction[1] * 100

uniform_under_pred_rel_wis <- (tab_uniform$underprediction[2] - tab_uniform$underprediction[1]) / tab_uniform$interval_score[1] * 100

uniform_over_pred_val <- tab_uniform$overprediction[2] - tab_uniform$overprediction[1]

uniform_over_pred_rel_wis <- (tab_uniform$overprediction[2] - tab_uniform$overprediction[1]) / tab_uniform$interval_score[1] * 100


# QSA Flexible_Symmetric
tab_flexible_symmetric <- uk_qsa_flexible_symmetric |>
  extract_validation_set() |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = c("method")) |>
  dplyr::select(method:dispersion, underprediction, overprediction)

flexible_symmetric_wis_rel <- (tab_flexible_symmetric$interval_score[2] - tab_flexible_symmetric$interval_score[1]) / tab_flexible_symmetric$interval_score[1] * 100

flexible_symmetric_dis_fac <- tab_flexible_symmetric$dispersion[2] / tab_flexible_symmetric$dispersion[1]

flexible_symmetric_under_pred_rel <- (tab_flexible_symmetric$underprediction[2] - tab_flexible_symmetric$underprediction[1]) / tab_flexible_symmetric$underprediction[1] * 100

flexible_symmetric_under_pred_val <- tab_flexible_symmetric$underprediction[2] - tab_flexible_symmetric$underprediction[1]

flexible_symmetric_over_pred_rel <- (tab_flexible_symmetric$overprediction[2] - tab_flexible_symmetric$overprediction[1]) / tab_flexible_symmetric$overprediction[1] * 100

flexible_symmetric_under_pred_rel_wis <- (tab_flexible_symmetric$underprediction[2] - tab_flexible_symmetric$underprediction[1]) / tab_flexible_symmetric$interval_score[1] * 100

flexible_symmetric_over_pred_val <- tab_flexible_symmetric$overprediction[2] - tab_flexible_symmetric$overprediction[1]

flexible_symmetric_uniform_over_pred_rel_wis <- (tab_flexible_symmetric$overprediction[2] - tab_flexible_symmetric$overprediction[1]) / tab_flexible_symmetric$interval_score[1] * 100


# QSA Flexible
tab_flexible <- uk_qsa_flexible |>
  extract_validation_set() |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = c("method")) |>
  dplyr::select(method:dispersion, underprediction, overprediction)

flexible_under_pred_rel <- (tab_flexible$underprediction[2] - tab_flexible$underprediction[1]) / tab_flexible$underprediction[1] * 100

flexible_over_pred_rel <- (tab_flexible$overprediction[2] - tab_flexible$overprediction[1]) / tab_flexible$overprediction[1] * 100
```

```{r, qsa-all-methods-table, echo=FALSE}
tab_qsa <- bind_rows(tab_uniform,tab_flexible_symmetric[2],tab_flexible[2]) |>
  rename(`interval score` = interval_score)

display_table(tab_qsa, caption="QSA Uniform improves WIS by increasing interval widths.", bold_header = TRUE, striped = FALSE)
```

## Aggregate

We begin by examining a high level overview of the results. \Cref{tab:qsa-all-methods-table} presents the performance of all three QSA flavors on the validation set, aggregated over all *models*, *target types*, *horizons* and *quantiles*. 

Starting with `qsa_uniform`, we observe clearl improvements in the WIS as it drops by `r round(uniform_wis_rel,2)` $\%$. As expected, overall, the post-processing increases the dispersion and hence the weighted prediction intervals by a factor of `r round(uniform_dis_fac,2)`.

The wider intervals thereby cover more observations which reduces the under- and overprediction by `r round(uniform_under_pred_rel,2)` $\%$. and `r round(uniform_over_pred_rel,2)` $\%$. Interestingly, while both decreases are similar in relative terms, there absolute effects differ substantially. The underprediction reduction decreases the WIS by `r round(uniform_under_pred_val,2)` which amounts to a relative WIS decrease of merely `r round(uniform_under_pred_rel_wis,2)` $\%$, while the overprediction drops by `r round(uniform_over_pred_val,2)` which is equal to a WIS drop by `r round(uniform_over_pred_rel_wis,2)` $\%$. The main driver behind the increase of the intervals, thus is there overcoverage. In other words: the intervals do not reach low enough. Overall, by increasing the intervals and achieving better coverage of smaller observations, while at the same time sacrificing interval sharpness, `qsa_uniform` improves the WIS.

Figure \Cref{fig:qsa-uniform-hor-quant} shows the WIS changes of `qsa_uniform` for each *horizon* and *quantile* combination, aggregated by *models* and *target types*. `qsa_uniform` is beneficial for extreme quantiles at large horizons. however it also substantially overfits extreme quantiles at the horizon of $1$. interestingly,  near to no changes can be observed for the smaller prediction intervals lying within the $0.25$ and $0.75$ quantiles.


```{r, qsa-uniform-hor-quant, echo=FALSE, out.width="80%", fig.cap="QSA Uniform beneficial for extreme Quantiles at large Horizons."}
df_hor_quant <- eval_methods(uk_qsa_uniform, summarise_by = c("horizon", "quantile"))
p1 <- plot_eval(df_hor_quant, base_size = 8) + ggplot2::labs(x = NULL)

p1
```

...

Figure \Cref{fig:qsa-uniform-hor-quant} revealed no significant adjustments for the inner confidence intervals. Due to the restriction of identical quantile spread adjustments for all quantiles, inherent to `qsa_uniform`, the optimization cannot differ in its post-processing of the various intervals. It could be the case that smaller intervals might need different adjustments than larger ones. This can be the case if humans have difficulty of intuitively grasping the concept of confidence intervals. In order to investigate this question, we examine the `qsa_flexible_symmetric` method. It allows the QSA adjustments to vary between intervals. Its only restriction is for adjustments to be symmetric, hence identical for each quantile pair, being the lower and upper bounds of symmetric intervals.

Table \Cref{tab:qsa-all-methods-table} presents the aggregated performance of `qsa_flexible_symmetric` on the validation set. The WIS remains lower in comparison to the original data, however it does lie above the v `qsa_uniform` by `r round(flexible_symmetric_wis_rel,2)` $\%$. in the aggregate `qsa_flexible_symmetric` seems to overfit compared to the much more restrictive `qsa_uniform`. Further evidence of overfitting are that the dispersion increases even further with a factor of `r round(flexible_symmetric_dis_fac,2)`, and that the underprediction as well as overprediction drop even lower with `r round(flexible_symmetric_under_pred_val,2)` and `r round(flexible_symmetric_over_pred_val,2)` $\%$ changes. 

In Figure \Cref{fig:qsa_flexible_symmetric-hor-quant} the WIS changes of `qsa_flexible_symmetric` for each *horizon* and *quantile* pair show how this more flexible method adjusted the different intervals. Suprisingly we see no changes in the inner quantiles between the $0.3$and $0.7$ quantiles. Apparently the intervals with coverages equal or smaller than $50$ $\%$ where already quite optimal in the original human forecasts. Furthermore, the gains for the larger intervals remain similar, which suggests that the restriction to adjust all intervals with the same quantile spread factor, did not pose an issue for the Uk data set. In contrast, we rather observe an issue in the third horizon where more extreme quantile gains drop and extreme quantiles at lower horizon are overfitted by `qsa_flexible_symmetric` as the adjustments are worse than originally.


```{r, qsa-flexible-symmetric-hor-quant, echo=FALSE, out.width="80%", fig.cap="QSA Flexible Symmetric overfits Quantiles at short Horizons."}
df_hor_quant <- eval_methods(uk_qsa_flexible_symmetric, summarise_by = c("horizon", "quantile"))
p1 <- plot_eval(df_hor_quant, base_size = 8) + ggplot2::labs(x = NULL)

p1
```



`qsa_uniform` and `qsa_flexible_symmetric` are both bound to symmetrically adjust upper and lower bounds of the prediction intervals. This is sensible for adjusting models who's residuals follow a symmetric distribution. If model residuals are however skewed, and thus interval coverage lacks more heavily on one side, symmetric adjustments lead to sub-optimal results. This happens because the model is confronted with a trade off where it adjusts one side to little and the other side to much. In the case where the post-processing increases intervals it is bound by the dispersion penalty that is heavier, since for each step it takes at reducing undercoverage, e.g. underprediction or overprediction, on one end, it increases dispersion two fold as intervals are also increased in the other end. In the case of decreasing intervals, it is bound by a lack of coverage as for each step it decreases unnecessary large intervals on one side, it also decreases the interval on the other side leading to uncovered observations. Thus, in both cases where post-processing is warranted, but the model residuals are non-symmetrical, symmetric methods lead to sub-optimal adjustments on both sides of the interval.
As the Covid-19 infection and death data is inherently non-symmetrically distributed, due to the observations being bounded between $[0,Inf]$ and them resulting from exponential growth, we expect model residuals to be skewed towards higher values. Therefor, we examine how the non-symmetric post-processing method `qsa_flexible` adjusts the forecasts and how it preforms in contrast to `qsa_uniform` and `qsa_flexible_symmetric`. 


\Cref{tab:qsa-all-methods-table} presents the aggregated performance of `qsa_flexible` on the validation set. The WIS is a clear improvement in comparison to the original data and lies in between the `qsa_uniform` and `qsa_flexible_symmetric`. Thus, it preforms slightly better than the `qsa_uniform` and slightly worse than the `qsa_flexible_symmetric` methods. Our main interest however lies in how intervals are adjusted, thus in the dispersion, underprediction and overprediction. The dispersion increases after post-processing, however to a lesser degree than for the other methods. The underprediction, most notably and in contrast to the symmetric approaches, substantially increases by `r round(flexible_under_pred_rel,2)` $\%$, while still remaning the lowest of the three WIS components. The overprediction behaves similarly to the  `qsa_flexible_symmetric` method and decreases strongly by `r round(flexible_over_pred_rel,2)` $\%$.
Due to the unsymmetrical nature of the misscoverage, in the aggregate, `qsa_flexible` moves the intervals downward, by heavily decreasing the lower quantiles in order to reduce overprediction and slightly decreasing the upper quantiles as the lost coverage is more than compensated by a reduction in dispersion. Surprisingly, due to the nature of exponential growth we would have expected human forecasters to underestimate trends, however for the UK Data, we observe an overconfidence in increasing cases and the death tool. 


## Across Models

```{r, qsa-all-methods-models, echo=FALSE}
tab_original_model <- uk_qsa_uniform |>
  extract_validation_set() |>
  scoringutils::score() |>
  dplyr::filter(method == "original") |>
  scoringutils::summarise_scores(by = c("model")) |>
  dplyr::select(model:interval_score, dispersion, underprediction, overprediction)

model_col <- tab_original_model$model
tab_original_model <- tab_original_model |>
  select(-model)


tab_uniform_model <- uk_qsa_uniform |>
  extract_validation_set() |>
  scoringutils::score() |>
  dplyr::filter(method == "qsa_uniform") |>
  scoringutils::summarise_scores(by = c("model")) |>
  dplyr::select(interval_score, dispersion, underprediction, overprediction)

tab_uniform_model_changes <- (tab_uniform_model - tab_original_model) / tab_original_model * 100
tab_uniform_model_changes$model <- model_col
tab_uniform_model_changes$method <- "uniform"

#Flexible Symmetric
tab_flexible_symmetric_model <- uk_qsa_flexible_symmetric |>
  extract_validation_set() |>
  scoringutils::score() |>
  dplyr::filter(method == "qsa_flexible_symmetric") |>
  scoringutils::summarise_scores(by = c("model")) |>
  dplyr::select(interval_score, dispersion, underprediction, overprediction)

tab_flexible_symmetric_model_changes <- (tab_flexible_symmetric_model - tab_original_model) / tab_original_model * 100
tab_flexible_symmetric_model_changes$model <- model_col
tab_flexible_symmetric_model_changes$method <- "symmetric"


# Flexible
tab_flexible_model <- uk_qsa_flexible |>
  extract_validation_set() |>
  scoringutils::score() |>
  dplyr::filter(method == "qsa_flexible") |>
  scoringutils::summarise_scores(by = c("model")) |>
  dplyr::select(interval_score, dispersion, underprediction, overprediction)

tab_flexible_model_changes <- (tab_flexible_model - tab_original_model) / tab_original_model * 100
tab_flexible_model_changes$model <- model_col
tab_flexible_model_changes$method <- "flexible"


tab_qsa_model_changes <- bind_rows(tab_uniform_model_changes, tab_flexible_symmetric_model_changes, tab_flexible_model_changes) |>
  rename(wis = interval_score, dis=dispersion, under=underprediction, over=overprediction) |>
  select(model, method, everything()) |>
  arrange(
    match(model, c("EuroCOVIDhub-baseline", "EuroCOVIDhub-ensemble", "epiforecasts-EpiExpert", "epiforecasts-EpiExpert_Rt", "epiforecasts-EpiExpert_direct", "seabbs")),
    match(method, c("uniform", "symmetric", "flexible")), 
    wis, dis, under, over)

#display_table(tab_qsa_model_changes, caption="QSA Methods differ in preformance across Models.", bold_header = TRUE, striped = FALSE)
```

\Cref{tab:qsa-all-methods-models} shows the results of QSA post-processing for all three methods by model. These more granular results reveal a pattern not visible in the aggregated results. the `qsa_flexible` method preforms significantly worse in the `EuroCOVIDhub-baseline` model by increasing the WIS by `r round(tab_qsa_model_changes[3,3][[1]],2)` $\%$. It overfits the training set as becomes evident by figure \Cref{fig:qsa-flexible-eu-baseline-intervals}. The figure shows that the original prediction intervals are below the actual values in the training period, then adjust to overshoot the actual values and level out during the validation period. As `qsa_flexible` is able to  adjust the intervals in a non-symmetrical manner, it learns to push both interval bounds upward during the training set. As this pattern of underprediction changes in the validation set and the QSA metrics equally weigh all observations, `qsa_flexible` takes some observations to adjust properly and overpredicts the observations in the meantime. In contrast, `qsa_uniform` and `qsa_flexible_symmetric`, although the also don't improve the WIS in the validation set, overfit much less due to there restraint of making symmetric adjustments.

```{r, qsa-flexible-eu-baseline-intervals, echo=FALSE, out.width="80%", fig.cap="QSA Flexible overfits as it intervals are to low in the training and to high in the validation set."}
p1 <- plot_intervals(
  uk_qsa,
  model = "EuroCOVIDhub-baseline", target_type = "Cases", horizon = 3, quantile = 0.2
) 

p1
```

For the `EuroCOVIDhub-ensemble` model we observe that the `qsa_uniform` method has the best performance and could reduce the WIS by `r round(tab_qsa_model_changes[4,3][[1]],2)`. It seems that a simple, quite restrictive uniform adjustment across all quantiles provided the largest benefit. Adding additional flexibility among intervals with `qsa_flexible_symmetric` actually reduced the gains by about half and the further flexibility of `qsa_flexible` with non-symmetric adjustments even lead to a slightly worse prediction. These results are quite encouraging as the `EuroCOVIDhub-ensemble` model represents an ensemble of modelling approaches by professional forecasting models and thus isn't burdened by human overconfidence.
For the human forecasting models, namely `epiforecasts-EpiExpert`, `epiforecasts-EpiExpert_Rt`, `epiforecasts-EpiExpert_direct` and `seabbs`, we observe that all QSA methods significantly improve the WIS. Furthermore for each model, there is at least one method that can reduce the Score by over $10 \%$. Regarding the last three models we even see a similar pattern among post processing method performances: `qsa_flexible` reduces the WIS most, followed by `qsa_uniform` and `qsa_flexible_symmetric`. For the first method, this ranking is reversed, however, the scores only vary slightly. An inspection of the WIS components provides further interesting observations: `qsa_flexible` consistently reduces overprediction the most, is the only method that increases underprediction and has the lowest increase in dispersion. These observations are the result of the non-symmetric adjustments which allow `qsa_flexible` to reduce the lower bound without having to increase the upper one. For the optimization this has two effects: For one, it can decrease the lower bound much stronger as the cost of doing so, in terms of dispersion, are cut in half. Second, it can now freely adjust the upper quantile reduce its value until the increase in underprediction is balanced out with the reduction in dispersion.

## Across Target Types

Comparing the QSA methods across target types reveals notable differences. Figure \Cref{fig:qsa-all-methods-target-types} shows the relative changes in WIS after applying `qsa_uniform`, `qsa_flexible_symmetric` and `qsa_flexible` to the original data broken down by the `target_type`. In the aggregate all three methods improve the score for both target types within a similar range. `qsa_flexible_symmetric` preforms best for `Deaths` and `qsa_uniform` for `Cases`.

```{r, qsa-all-methods-target-types, echo=FALSE, out.width="80%", fig.cap="Across both Target Types all QSA mrthods improve the WIS."}
df_target <- eval_methods(uk_qsa, summarise_by = "target_type") |>
  select(target_type,qsa_uniform,qsa_flexible_symmetric,qsa_flexible)
p1 <- plot_eval(df_target, base_size = 8) + ggplot2::labs(x = NULL)
p1
```

If we split models into human and model forecasts the results change as is depicted in Figure \Cref{fig:qsa-all-methods-target-types-model-groups}. Human forecasts primarily benefit from post processing for the `target_type` `Cases`, while model forecasts are only improved in their `Deaths` predictions. For both major improvements, as discussed regarding the human forcasted models, `qsa_flexible` reduces the WIS most, followed by `qsa_uniform` and `qsa_flexible_symmetric`. in terms of overfitting, we observe that `qsa_flexible` is the only model that increases the score most. these results once more illustrate that `qsa_flexible` is a riskier model, as it can lead to higher gains or losses due to its potential to more closely fit the training data.

```{r, qsa-all-methods-target-types-model-groups, echo=FALSE, out.width="80%", fig.cap="Forecasting improvements differ across target types for different model groups. Human forecasts are primarily improved for cases, while model forecast improvments are only found for deaths."}
uk_qsa_human <- uk_qsa |>
  dplyr::filter(model %in% c("epiforecasts-EpiExpert", "epiforecasts-EpiExpert_Rt", "epiforecasts-EpiExpert_direct", "seabbs")) #only human forecast

df_target_human <- eval_methods(uk_qsa_human, summarise_by = "target_type") |>
  select(target_type,qsa_uniform,qsa_flexible_symmetric,qsa_flexible)
p1 <- plot_eval(df_target_human, base_size = 8) + ggplot2::labs(x = NULL) + ggtitle("Human Forecasts") + ggplot2::labs(subtitle = "")

uk_qsa_forecaster <- uk_qsa |>
  dplyr::filter(model %in% c("EuroCOVIDhub-baseline", "EuroCOVIDhub-ensemble")) #only forecasting models

df_target_forecasters <- eval_methods(uk_qsa_forecaster, summarise_by = "target_type") |>
  select(target_type,qsa_uniform,qsa_flexible_symmetric,qsa_flexible)
p2 <- plot_eval(df_target_forecasters, base_size = 8) + ggplot2::labs(x = NULL) + ggtitle("Model Forecasts") + ggplot2::labs(subtitle = "")

p1 + p2 

```

## Across Horizons

Breaking down the results by the forecasting `horizon` also reveals notable patterns. Figure \Cref{fig:qsa-all-models-horizons} plots the WIS changes across horizons for all three methods. Across all QSA methods the improvements to the WIS increase with the `horizon`. The gains are primarily visible for the three and four week ahead predictions. In Contrast the increases in score and overfitting are primarily located at a horizon of one. 

```{r, qsa-all-models-horizons, echo=FALSE, out.width="80%", fig.cap="QSA methods improve forecasts more for larger horizons. For smaller horizons they tend to voerfit, this is especially the case with qsa_flexible."}
df_hor <- eval_methods(uk_qsa, summarise_by = "horizon") |>
  select(horizon,qsa_uniform,qsa_flexible_symmetric,qsa_flexible)
p1 <- plot_eval(df_hor, base_size = 8) + ggplot2::labs(x = NULL)

p1
```

Again, a split of the post processed models into human and model forecasts reveals differences as depicted in Figure \Cref{qsa-all-models-horizons-model-groups}. We observe that the aggregate gains soley stem from the human forecast models and that the losses in the WIS are primarily from the model forecasts. Here the method performances also vary more. The largest gains and losses are reported for `qsa_flexible`, while `qsa_uniform` and `qsa_flexible_symmetric` also report improvements but overfit much less.

```{r, qsa-all-models-horizons-model-groups, echo=FALSE, out.width="80%", fig.cap="Forecasting improvements differ across horizons for different model groups. Human forecasts are primarily improved for horizons larger than 2, while model forecast are not improved at all and are overfitted with qsa_flexible."}
uk_qsa_human <- uk_qsa |>
  dplyr::filter(model %in% c("epiforecasts-EpiExpert", "epiforecasts-EpiExpert_Rt", "epiforecasts-EpiExpert_direct", "seabbs")) #only human forecast

df_target_human <- eval_methods(uk_qsa_human, summarise_by = "horizon") |>
  select(horizon,qsa_uniform,qsa_flexible_symmetric,qsa_flexible)
p1 <- plot_eval(df_target_human, base_size = 8) + ggplot2::labs(x = NULL) + ggtitle("Human Forecasts") + ggplot2::labs(subtitle = "")

uk_qsa_forecaster <- uk_qsa |>
  dplyr::filter(model %in% c("EuroCOVIDhub-baseline", "EuroCOVIDhub-ensemble")) #only forecasting models

df_target_forecasters <- eval_methods(uk_qsa_forecaster, summarise_by = "horizon") |>
  select(horizon,qsa_uniform,qsa_flexible_symmetric,qsa_flexible)
p2 <- plot_eval(df_target_forecasters, base_size = 8) + ggplot2::labs(x = NULL) + ggtitle("Model Forecasts") + ggplot2::labs(subtitle = "")

p1 + p2 

```

Additionally breaking down the results by target types as well, reveals the patterns in Figures \Cref{qsa-all-models-horizons-models-target-types} and \Cref{qsa-all-models-horizons-humans-target-types} of the appendix. They show gains for death and losses for case predictions across the board for model forecasts. Again, these tendencies are strongest for the `qsa_flexible` flavor of QSA. Human predictions are primarily improved for cases and the horizons of 3 and 4 weeks ahead. For deaths `qsa_flexible` worsens the score, especially for shorter horizons while `qsa_uniform` and `qsa_flexible_symmetric` slightly improve the scores across all horizons.

Overall there is a tendency that forecast improvements increase and and the risk of overfitting simultaneously drops with increasing forecasting horizons.

## Across Quantiles

WIS improvements also vary across the different quantiles and intervals. Figure \Cref{fig:qsa-all-models-quantiles} depicts the WIS across quantiles for the three QSA flavors. the improvements are larger for more extreme quantiles.


```{r, qsa-all-models-quantiles, echo=FALSE, out.width="80%", fig.cap="QSA improves forecasters larger for more extreme qunatiles and thus larger intervals."}
df_quant <- eval_methods(uk_qsa, summarise_by = "quantile") |>
  select(quantile,qsa_uniform,qsa_flexible_symmetric,qsa_flexible)
p1 <- plot_eval(df_quant, base_size = 8) + ggplot2::labs(x = NULL)

p1
```

As we have shown that aggregation across the `target_type`and `model` dimensions do not show the full picture, we also show the quantile improvements for the human forecasts of cases as well as the model forecasts of deaths in Figure \Cref{fig:qsa-quantiles-model-groups-target-types-best}. For human forecasts of cases the patterns remain similar to the aggregate. For model forecasts of deaths however, we observe larger improvements and that `qsa_flexible` is useful for small intervals. This results from the death prediction intervals beenfitting from non-symmetric adjustments. Furthermore model forecasts of deaths also seem to be one of the rare cases where `qsa_flexible_symmetric` outpreforms `qsa_uniform`.

```{r, qsa-quantiles-model-groups-target-types-best, echo=FALSE, out.width="80%", fig.cap="QSA Flexible overfits as it intervals are to low in the training and to high in the validation set."}
uk_qsa_human_cases <- uk_qsa |>
  dplyr::filter(model %in% c("epiforecasts-EpiExpert", "epiforecasts-EpiExpert_Rt", "epiforecasts-EpiExpert_direct", "seabbs"), target_type == "Cases") #only human forecast

df_hor_human_cases <- eval_methods(uk_qsa_human_cases, summarise_by = "quantile") |>
  select(quantile,qsa_uniform,qsa_flexible_symmetric,qsa_flexible)
p1 <- plot_eval(df_hor_human_cases, base_size = 8) + ggplot2::labs(x = NULL) + ggtitle("Human Forecasts of Cases") + ggplot2::labs(subtitle = "")

uk_qsa_forecaster <- uk_qsa |>
  dplyr::filter(model %in% c("EuroCOVIDhub-baseline", "EuroCOVIDhub-ensemble"), target_type == "Deaths") #only model forecast

df_hor_forecaster_death <- eval_methods(uk_qsa_forecaster, summarise_by = "quantile") |>
  select(quantile,qsa_uniform,qsa_flexible_symmetric,qsa_flexible)
p2 <- plot_eval(df_hor_forecaster_death, base_size = 8) + ggplot2::labs(x = NULL) + ggtitle("Model Forecasts of Deaths") + ggplot2::labs(subtitle = "")


p1 + p2
```

Further examination of the results, in particular subsetting the above results to the `horizon`of three and four revealed an exception to the accordion look of the quantile graphs. For large forecasting horizons and model forecasts of deaths, we observe worse WIS after the adjustments. These result from the high cost of not covering an observation at extreme quantiles. Figure \Cref{fig:qsa-large-quantiles-no-coverage} exemplifies this where all QSA methods substantially reduces the interval sizes in order to reduce dispersion, this then  result in undercoverage of the last week of august. Thus the QSA adjustments, especially for few data points ,as the 13 weeks of the UK data, can underestimate uncertainty at extreme quantiles. This risk increases with the flexibility of the QSA flavor.


```{r, qsa-large-quantiles-no-coverage, echo=FALSE, out.width="80%", fig.cap="QSA can underestimate uncertainty for extreme qunatiles and few data points to learn from."}
uk_qsa |>
  dplyr::filter(model %in% c("EuroCOVIDhub-ensemble"), target_type == "Deaths", horizon %in% c(3), quantile %in% c(0.01,0.99)) |>
  eval_methods(summarise_by="quantile")

p1 <- plot_intervals(
  uk_qsa,
  model = "EuroCOVIDhub-ensemble", target_type = "Deaths", horizon = 4, quantile = 0.01
) 

p1
```

## Conclusion

- qsa uniform overall most consistent model
- symmetric flexible didnt bring notable gains
- qsa flexible works better or worse depending on the case
- human forecaster models more improved ofr cases and forecaster models rather for deaths
- regarding horizons and qunatile in general the further out the better

- talk about improvements and next steps

This finding, of the post processing methods increasing intervals, confirms the hypothesis that humans tend to be too confident in their own forecasts leading to narrow prediction intervals.
