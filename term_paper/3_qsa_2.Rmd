---
output:
  bookdown::pdf_document2:
    includes:
      in_header: preamble.tex
    toc: FALSE
    highlight: tango
    number_sections: TRUE
    df_print: tibble
    latex_engine: pdflatex
bibliography: [paper.bib, packages.bib]
biblio-style: apalike
urlcolor: black
linkcolor: blue
links-as-notes: true
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE, fig.align = "center",
  out.width = "100%", dpi = 300, fig.asp = 0.618, collapse = TRUE
)

devtools::load_all(".")
library(dplyr)
library(ggplot2)
library(patchwork)

uk_qsa_uniform <- readr::read_rds(here::here("data_results", "uk_qsa_uniform.rds"))
uk_qsa_flexible_symmetric <- readr::read_rds(here::here("data_results", "uk_qsa_flexible_symmetric.rds"))
uk_qsa_flexible <- readr::read_rds(here::here("data_results", "uk_qsa_flexible.rds"))
```

## Results

As for the CQR method, we investigate how well QSA performs for post-processing Covid-19 forecasts. We mainly focus on the UK Covid-19 Forecasting Challenge data set and only mention `qsa_uniform` results in the European Forecast Hub data due to computational restrictions.

```{r, tab:qsa_uniform-val-set, echo=FALSE}
tab_uniform <- uk_qsa_uniform |>
  extract_validation_set() |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = c("method")) |>
  dplyr::select(method:dispersion, underprediction, overprediction)

tab_uniform
```

We begin by examining the results of the `qsa_uniform` method and taking a high level view. \Cref{tab:qsa_uniform-val-set} presents the performance on the validation set, aggregated over all *models*, *target types*, *horizons* and *quantiles*. `qsa_uniform` clearly improves the Weighted Interval Score as it drops by `r (tab_uniform$interval_score[1] - tab_uniform$interval_score[2]) / tab_uniform$interval_score[1] * 100` percent. As expected post-processing makes the prediction intervals larger as the dispersion increases by a factor of `r tab_uniform$dispersion[2] / tab_uniform$dispersion[1]`. 

The increased intervals cover more observations and thereby reduce the under- and overprediction by `r (tab_uniform$underprediction[1] - tab_uniform$underprediction[2]) / tab_uniform$underprediction[1] * 100` and `r (tab_uniform$overprediction[1] - tab_uniform$overprediction[2]) / tab_uniform$overprediction[1] * 100`. Interestingly while both decreases are similar in terms of relative performance increases, there absolute effects on the interval score differ substantially. The underprediction reduction decreases the WIS by `r tab_uniform$underprediction[1] - tab_uniform$underprediction[2]` which amounts to a relative decrease of merely `r (tab_uniform$underprediction[1] - tab_uniform$underprediction[2]) / tab_uniform$interval_score[1] * 100` percent, while the overprediction drops by `r tab_uniform$overprediction[1] - tab_uniform$overprediction[2]` which in relativ terms are `r (tab_uniform$overprediction[1] - tab_uniform$overprediction[2]) / tab_uniform$interval_score[1] * 100` percent. The main driver behind the increasing in the intervals, is that they do not reach high enough. Thus by increasing the intervals and achieving better coverage of larger observations, while at the same time sacrificing interval sharpness, `qsa_uniform` improves the WIS.

This finding, of the post processing methods increasing intervals, confirms the hypothesis that humans tend to be too confident in their own forecasts leading to narrow prediction intervals.


\Cref{tab:qsa_flexible-val-set} presents the aggregated performance of `qsa_flexible` on the validation set.


```{r, tab:qsa_flexible_symmetric-val-set, echo=FALSE}
uk_qsa_flexible_symmetric |>
  extract_validation_set() |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = c("method")) |>
  dplyr::select(method:dispersion, underprediction, overprediction)
```

`qsa_uniform` and `qsa_flexible_symmetric` are both bound to symmetrically adjust upper and lower bounds of the prediction intervals. This is sensible for adjusting models who's residuals follow a symmetric distribution. If model residuals are however skewed, and thus interval coverage lacks more heavily on one side, symmetric adjustments lead to sub-optimal results. This happens because the model is confronted with a trade off where it adjusts one side to little and the other side to much. In the case where the post-processing increases intervals it is bound by the dispersion penalty that is heavier, since for each step it takes at reducing undercoverage, e.g. underprediction or overprediction, on one end, it increases dispersion two fold as intervals are also increased in the other end. In the case of decreasing intervals, it is bound by a lack of coverage as for each step it decreases unnecessary large intervals on one side, it also decreases the interval on the other side leading to uncovered observations. Thus, in both cases where post-processing is warranted, but the model residuals are non-symmetrical, symmetric methods lead to sub-optimal adjustments on both sides of the interval.
As the Covid-19 infection and death data is inherently non-symmetrically distributed, due to the observations being bounded between $[0,Inf]$ and them resulting from exponential growth, we expect model residuals to be skewed towards higher values. Therefor, we examine how the non-symmetric post-processing method `qsa_flexible` adjusts the forecasts and how it preforms in contrast to `qsa_uniform` and `qsa_flexible_symmetric`. 

\Cref{tab:qsa_flexible-val-set} presents the aggregated performance of `qsa_flexible` on the validation set. The WIS is a clear improvement in comparison to the original data and lies in between the `qsa_uniform` and `qsa_flexible_symmetric`. Thus, it preforms slightly better than the `qsa_uniform` and slightly worse than the `qsa_flexible_symmetric` methods. Our main interest however lies in how intervals are adjusted, thus in the dispersion, underprediction and overprediction. The dispersion increases after post-processing, however to a lesser degree than for the other methods. The underprediction, most notably and in contrast to the symmetric approaches, substantially increases by `r (tab_flexible$underprediction[1] - tab_flexible$underprediction[2]) / tab_flexible$underprediction[1] * 100` percent, while still remaning the lowest of the three WIS components. The overprediction behaves similarly to the  `qsa_flexible_symmetric` method and decreases strongly by `r (tab_flexible$overprediction[1] - tab_flexible$overprediction[2]) / tab_flexible$overprediction[1] * 100` percent.
Due to the unsymmetrical nature of the misscoverage, in the aggregate, `qsa_flexible` moves the intervals downward, by heavily decreasing the lower quantiles in order to reduce overprediction and slightly decreasing the upper quantiles as the lost coverage is more than compensated by a reduction in dispersion. Surprisingly, due to the nature of exponential growth we would have expected human forecasters to underestimate trends, however for the UK Data, we observe an overconfidence in increasing cases and the death tool. 

```{r, tab:qsa_flexible-val-set, echo=FALSE}
tab_flexible <- uk_qsa_flexible |>
  extract_validation_set() |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = c("method")) |>
  dplyr::select(method:dispersion, underprediction, overprediction)

tab_flexible
```