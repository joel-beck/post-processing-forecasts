---
output:
  bookdown::pdf_document2:
    includes:
      in_header: preamble.tex
    toc: FALSE
    highlight: tango
    number_sections: TRUE
    df_print: tibble
    latex_engine: pdflatex
bibliography: [paper.bib, packages.bib]
biblio-style: apalike
urlcolor: black
linkcolor: blue
links-as-notes: true
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE, fig.align = "center",
  out.width = "100%", dpi = 300, fig.asp = 0.618, collapse = TRUE
)

devtools::load_all(".")
library(dplyr)
library(ggplot2)
library(patchwork)

uk_qsa_uniform <- readr::read_rds(here::here("data_results", "uk_qsa_uniform.rds"))
uk_qsa_flexible_symmetric <- readr::read_rds(here::here("data_results", "uk_qsa_flexible_symmetric.rds"))
uk_qsa_flexible <- readr::read_rds(here::here("data_results", "uk_qsa_flexible.rds"))

# TODO: add display table function with captions
# TODO: add empty spaces, less block text.
```

```{r, include=FALSE}
display_table <- function(df, caption, bold_header = TRUE, striped = FALSE) {
  tab <- df |>
    kableExtra::kbl(
      digits = 2, align = "c", booktabs = TRUE, caption = caption
    ) |>
    kableExtra::row_spec(row = 0, bold = bold_header) |>
    kableExtra::kable_styling(position = "center", full_width = FALSE)

  if (striped) {
    tab <- tab |> kableExtra::kable_styling(latex_options = "striped")
  }

  return(tab)
}
```


## Results

As for the CQR method, we investigate how well QSA performs for post-processing Covid-19 forecasts. We mainly focus on the UK Covid-19 Forecasting Challenge data set and only mention `qsa_uniform` results in the European Forecast Hub data due to computational restrictions.

```{r, qsa-uniform-val-set, echo=FALSE}
tab_uniform <- uk_qsa_uniform |>
  extract_validation_set() |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = c("method")) |>
  dplyr::select(method:dispersion, underprediction, overprediction) |>
  rename(`interval score` = interval_score) #|>
# rename(`dispersion` = dispersion) |>
# rename(`underprediction` = underprediction) |>
# rename(`overprediction` = overprediction)


uniform_wis_rel <- (tab_uniform$interval_score[2] - tab_uniform$interval_score[1]) / tab_uniform$interval_score[1] * 100

uniform_dis_fac <- tab_uniform$dispersion[2] / tab_uniform$dispersion[1]

uniform_under_pred_rel <- (tab_uniform$underprediction[2] - tab_uniform$underprediction[1]) / tab_uniform$underprediction[1] * 100

uniform_under_pred_val <- tab_uniform$underprediction[2] - tab_uniform$underprediction[1]

uniform_over_pred_rel <- (tab_uniform$overprediction[2] - tab_uniform$overprediction[1]) / tab_uniform$overprediction[1] * 100

uniform_under_pred_rel_wis <- (tab_uniform$underprediction[2] - tab_uniform$underprediction[1]) / tab_uniform$interval_score[1] * 100

uniform_over_pred_val <- tab_uniform$overprediction[2] - tab_uniform$overprediction[1]

uniform_over_pred_rel_wis <- (tab_uniform$overprediction[2] - tab_uniform$overprediction[1]) / tab_uniform$interval_score[1] * 100

display_table(
  tab_uniform,
  caption = "QSA Uniform improves WIS by increasing interval widths.", bold_header = TRUE, striped = FALSE
)
```

We begin by examining the results of the `qsa_uniform` method and taking a high level view. \Cref{tab:qsa-uniform-val-set} presents the performance on the validation set, aggregated over all *models*, *target types*, *horizons* and *quantiles*. `qsa_uniform` clearly improves the Weighted Interval Score as it drops by `r uniform_wis_rel` percent. As expected post-processing makes the prediction intervals larger as the dispersion increases by a factor of `r uniform_dis_fac`. 

The increased intervals cover more observations and thereby reduce the under- and overprediction by `r uniform_under_pred_rel` and `r uniform_over_pred_rel`. Interestingly while both decreases are similar in terms of relative performance increases, there absolute effects on the interval score differ substantially. The underprediction reduction decreases the WIS by `r uniform_under_pred_val` which amounts to a relative decrease of merely `r uniform_under_pred_rel_wis` percent, while the overprediction drops by `r uniform_over_pred_val` which in relativ terms are `r uniform_over_pred_rel_wis` percent. The main driver behind the increasing in the intervals, is that they do not reach high enough. Thus by increasing the intervals and achieving better coverage of larger observations, while at the same time sacrificing interval sharpness, `qsa_uniform` improves the WIS.

This finding, of the post processing methods increasing intervals, confirms the hypothesis that humans tend to be too confident in their own forecasts leading to narrow prediction intervals.

Figure \Cref{fig:qsa-uniform-hor-quant} shows the WIS changes of `qsa_uniform` for each *horizon* and *quantile* combination, aggregated by *models* and *target types*. `qsa_uniform` is beneficial for extreme quantiles at large horizons. however it also substantially overfits extreme quantiles at the horizon of $1$. interestingly,  near to no changes can be observed for the smaller prediction intervals lying within the $0.25$ and $0.75$ quantiles.


```{r, qsa-uniform-hor-quant, echo=FALSE, out.width="70%", fig.cap="QSA Uniform beneficial for extreme Quantiles at large Horizons."}
df_hor_quant <- eval_methods(uk_qsa_uniform, summarise_by = c("horizon", "quantile"))
p1 <- plot_eval(df_hor_quant, base_size = 8) + ggplot2::labs(x = NULL)

p1
```

...

Figure \Cref{fig:qsa-uniform-hor-quant} revealed no significant adjustments for the inner confidence intervals. Due to the restriction of identical quantile spread adjustments for all quantiles, inherent to `qsa_uniform`, the optimization cannot differ in its post-processing of the various intervals. It could be the case that smaller intervals might need different adjustments than larger ones. This can especially be the case if humans have difficulty of intuitively grasping the concept of confidence intervals. In order to investigate this question, we examines the `qsa_flexible_symmetric` method. It allows the QSA adjustments to vary between intervals. Its only restriction is for adjustments to be symmetric, hence identical for each quantile pair, being the lower and upper bounds of symmetric intervals.

```{r, qsa-flexible-symmetric-val-set, echo=FALSE}
tab_flexible_symmetric <- uk_qsa_flexible_symmetric |>
  extract_validation_set() |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = c("method")) |>
  dplyr::select(method:dispersion, underprediction, overprediction)

flexible_symmetric_wis_rel <- (tab_flexible_symmetric$interval_score[2] - tab_flexible_symmetric$interval_score[1]) / tab_flexible_symmetric$interval_score[1] * 100

flexible_symmetric_dis_fac <- tab_flexible_symmetric$dispersion[2] / tab_flexible_symmetric$dispersion[1]

flexible_symmetric_under_pred_rel <- (tab_flexible_symmetric$underprediction[2] - tab_flexible_symmetric$underprediction[1]) / tab_flexible_symmetric$underprediction[1] * 100

flexible_symmetric_under_pred_val <- tab_flexible_symmetric$underprediction[2] - tab_flexible_symmetric$underprediction[1]

flexible_symmetric_over_pred_rel <- (tab_flexible_symmetric$overprediction[2] - tab_flexible_symmetric$overprediction[1]) / tab_flexible_symmetric$overprediction[1] * 100

flexible_symmetric_under_pred_rel_wis <- (tab_flexible_symmetric$underprediction[2] - tab_flexible_symmetric$underprediction[1]) / tab_flexible_symmetric$interval_score[1] * 100

flexible_symmetric_over_pred_val <- tab_flexible_symmetric$overprediction[2] - tab_flexible_symmetric$overprediction[1]

flexible_symmetric_uniform_over_pred_rel_wis <- (tab_flexible_symmetric$overprediction[2] - tab_flexible_symmetric$overprediction[1]) / tab_flexible_symmetric$interval_score[1] * 100

display_table(
  tab_flexible_symmetric,
  caption = "QSA Flexible Symmetric improves WIS by increasing interval widths.", bold_header = TRUE, striped = FALSE
)
```

Table \Cref{tab:qsa-flexible-symmetric-val-set} presents the aggregated performance of `qsa_flexible_symmetric` on the validation set. The WIS remains lower in comparison to the original data, however it does lie above the v `qsa_uniform` by `r flexible_symmetric_wis_rel` percent. in the aggregate `qsa_flexible_symmetric` seems to overfit compared to the much more restrictive `qsa_uniform`. Further evidence of overfitting are that the dispersion increases even further with a factor of `r flexible_symmetric_dis_fac`, and that the underprediction as well as overprediction drop even lower with `r flexible_symmetric_under_pred_val` and `r flexible_symmetric_over_pred_val` percent changes. 

In Figure \Cref{fig:qsa-flexible-symmetric-hor-quant} the WIS changes of `qsa_flexible_symmetric` for each *horizon* and *quantile* pair show how this more flexible method adjusted the different intervals. Suprisingly we see no changes in the inner quantiles between the $0.3$and $0.7$ quantiles. Apparently the intervals with coverages equal or smaller than $50$ percent where already quite optimal in the original human forecasts. Furthermore, the gains for the larger intervals remain similar, which suggests that the restriction to adjust all intervals with the same quantile spread factor, did not pose an issue for the Uk data set. In contrast, we rather observe an issue in the third horizon where more extreme quantile gains drop and extreme quantiles at lower horizon are overfitted by `qsa_flexible_symmetric` as the adjustments are worse than originally.


```{r, qsa-flexible-symmetric-hor-quant, echo=FALSE, out.width="70%", fig.cap="QSA Flexible Symmetric overfits Quantiles at short Horizons."}
df_hor_quant <- eval_methods(uk_qsa_flexible_symmetric, summarise_by = c("horizon", "quantile"))
p1 <- plot_eval(df_hor_quant, base_size = 8) + ggplot2::labs(x = NULL)

p1
```



`qsa_uniform` and `qsa_flexible_symmetric` are both bound to symmetrically adjust upper and lower bounds of the prediction intervals. This is sensible for adjusting models who's residuals follow a symmetric distribution. If model residuals are however skewed, and thus interval coverage lacks more heavily on one side, symmetric adjustments lead to sub-optimal results. This happens because the model is confronted with a trade off where it adjusts one side to little and the other side to much. In the case where the post-processing increases intervals it is bound by the dispersion penalty that is heavier, since for each step it takes at reducing undercoverage, e.g. underprediction or overprediction, on one end, it increases dispersion two fold as intervals are also increased in the other end. In the case of decreasing intervals, it is bound by a lack of coverage as for each step it decreases unnecessary large intervals on one side, it also decreases the interval on the other side leading to uncovered observations. Thus, in both cases where post-processing is warranted, but the model residuals are non-symmetrical, symmetric methods lead to sub-optimal adjustments on both sides of the interval.
As the Covid-19 infection and death data is inherently non-symmetrically distributed, due to the observations being bounded between $[0,Inf]$ and them resulting from exponential growth, we expect model residuals to be skewed towards higher values. Therefore, we examine how the non-symmetric post-processing method `qsa_flexible` adjusts the forecasts and how it preforms in contrast to `qsa_uniform` and `qsa_flexible_symmetric`. 

```{r, qsa-flexible-val-set, echo=FALSE}
tab_flexible <- uk_qsa_flexible |>
  extract_validation_set() |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = c("method")) |>
  dplyr::select(method:dispersion, underprediction, overprediction)

flexible_under_pred_rel <- (tab_flexible$underprediction[2] - tab_flexible$underprediction[1]) / tab_flexible$underprediction[1] * 100

flexible_over_pred_rel <- (tab_flexible$overprediction[2] - tab_flexible$overprediction[1]) / tab_flexible$overprediction[1] * 100

display_table(tab_flexible, caption = "QSA Flexible improves WIS by moving intervals upward.", bold_header = TRUE, striped = FALSE)
```

\Cref{tab:qsa-flexible-val-set} presents the aggregated performance of `qsa_flexible` on the validation set. The WIS is a clear improvement in comparison to the original data and lies in between the `qsa_uniform` and `qsa_flexible_symmetric`. Thus, it preforms slightly better than the `qsa_uniform` and slightly worse than the `qsa_flexible_symmetric` methods. Our main interest however lies in how intervals are adjusted, thus in the dispersion, underprediction and overprediction. The dispersion increases after post-processing, however to a lesser degree than for the other methods. The underprediction, most notably and in contrast to the symmetric approaches, substantially increases by `r flexible_under_pred_rel` percent, while still remaning the lowest of the three WIS components. The overprediction behaves similarly to the  `qsa_flexible_symmetric` method and decreases strongly by `r flexible_over_pred_rel` percent.
Due to the unsymmetrical nature of the misscoverage, in the aggregate, `qsa_flexible` moves the intervals downward, by heavily decreasing the lower quantiles in order to reduce overprediction and slightly decreasing the upper quantiles as the lost coverage is more than compensated by a reduction in dispersion. Surprisingly, due to the nature of exponential growth we would have expected human forecasters to underestimate trends, however for the UK Data, we observe an overconfidence in increasing cases and the death tool. 
