---
output:
  bookdown::pdf_document2:
    includes:
      in_header: preamble.tex
    toc: FALSE
    highlight: tango
    number_sections: TRUE
    df_print: tibble
    latex_engine: pdflatex
bibliography: [paper.bib, packages.bib]
biblio-style: apalike
urlcolor: black
linkcolor: blue
links-as-notes: true
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE, fig.align = "center",
  out.width = "100%", dpi = 300, fig.asp = 0.618, collapse = TRUE
)

devtools::load_all(".")
library(dplyr)
library(ggplot2)
library(patchwork)

uk_complete <- readr::read_rds(here::here("data_results", "uk_complete.rds"))
```

```{r, include=FALSE}
display_table <- function(df, caption, bold_header = TRUE,
                          striped = FALSE) {
  tab <- df |>
    kableExtra::kbl(
      digits = 2, align = "c", booktabs = TRUE, caption = caption
    ) |>
    kableExtra::row_spec(row = 0, bold = bold_header) |>
    kableExtra::kable_styling(position = "center", full_width = FALSE)

  if (striped) {
    tab <- tab |> kableExtra::kable_styling(latex_options = "striped")
  }

  return(tab)
}
```

# Method Comparison {#comparison}

This chapter aims to compare the effectiveness of all post-processing methods that were introduced throughout previous chapters.
In particular, we investigate if some methods consistently *outperform* other procedures across a wide range of scenarios.
Further, it will be interesting to observe the *types* of adjustments to the original forecasts:
Some methods might improve the Weighted Interval Score by *extending* the interval width and thus increasing coverage, whereas others might yield a comparable final score by *shrinking* the prediction intervals leading to higher precision.
One can imagine even more variations: Moving the interval bounds farther apart or closer together can happen *symmetricly* or *asymmetricly* and the interval's midpoint might stay *fixed* or get *shifted* by the post-processing algorithm.

Before jumping into the analysis, we propose one additional model that, in contrast to those we have covered so far, does not add any new information to the equation.
Instead, it *combines* the predictions from existing post-processing methods to build an *ensemble* prediction.
The idea is that leveraging information from multiple independent algorithms can stabilize estimation since the ensemble learns to focus on the strongest individual model within each area of the feature space.
Next, we explain the mathematical reasoning behind the ensemble model in more detail.

## Ensemble Model {#ensemble}

There exist various options how to generally combine multiple building blocks into one ensemble.
We chose an approach that can be efficiently computed by well-understood optimization algorithms and, at the same time, is highly interpretable.
Each quantile prediction of our ensemble model is a *convex combination* of the individual methods, i.e. a linear combination where all weights are contained in the unit interval and sum up to one.
Hence, the resulting value lives on the same scale as the original predictions and each weight can be interpreted as the *fractional contribution* of the corresponding building block method. 

Consider one particular feature combination of `model`, `location`, `horizon`, `target_type` and `quantile`.
Let $n$ specify the number of observations in the training set within this combination, $\mathbf{y} \in \mathbb{R}^n$ the vector of true values, $\mathbf{l}_1, \ldots, \mathbf{l}_k \in \mathbb{R}^n$ vectors of original lower quantile predictions and $\mathbf{u}_1, \ldots, \mathbf{u}_k \in \mathbb{R}^n$ vectors of original upper quantile predictions from $k$ different post-processing procedures.

Then, for each such combination, the ensemble model computes weights $\mathbf{w}^* \in [0, 1]^k$ by solving the following nonlinear constrained optimization problem:
<!--  -->
$$
\begin{aligned}
\mathbf{w}^*
= \operatorname*{arg\,min}_{ \mathbf{w} \in [0, 1]^k} IS_\alpha(\mathbf{y})
&= \operatorname*{arg\,min}_{ \mathbf{w} \in [0, 1]^k} (\mathbf{u}-\mathbf{l}) + \frac{2}{\alpha} \cdot (\mathbf{l}-\mathbf{y}) \cdot \mathbbm{1} (\mathbf{y} \leq \mathbf{l}) + \frac{2}{\alpha} \cdot (\mathbf{y}-\mathbf{u}) \cdot \mathbbm{1}(\mathbf{y} \geq \mathbf{u}), \\
\text{with} \qquad \mathbf{l} &= \sum_{j=1}^{k} w_j \mathbf{l}_j, \;\; \mathbf{u} = \sum_{j=1}^{k} w_j \mathbf{u}_j \\
\text{s.t.} \qquad \left \Vert \mathbf{w} \right \Vert_1 &= \sum_{j=1}^{k} w_j = 1,
\end{aligned}
$$
<!--  -->
where all operations for vector inputs $\mathbf{l}$, $\mathbf{u}$ and $\mathbf{y}$ are understood elementwise and the *same* weights $w_j$, $j = 1, \ldots, k$ are chosen for lower and upper quantiles.  

Hence, we choose the (nonlinear) Interval Score (\Cref{wis}) as our objective function that we minimize subject to linear constraints.
The optimization step is implemented with the [nloptr](https://cran.r-project.org/web/packages/nloptr/index.html) package [@R-nloptr], which describes itself as "an R interface to NLopt, a free/open-source library for nonlinear optimization".

Note that, technically, the weight vector has to be denoted by $\mathbf{w}_{m, l, h, t, q}^*$ since the computed weights are generally different for each feature combination.
We omit the subscripts at this point to keep the notation clean.

The Interval Score always considers *pairs* of quantiles $\alpha$ and $1 - \alpha$ as outer bounds of a $(1 - 2 \alpha) \cdot 100\%$ prediction interval.
The best results are achieved when a separate weight vector for each quantile pair is computed.
Since our data sets contain $11$ quantile pairs, $2$ target types, $4$ horizons and we consider $6$ different forecasting models, the ensemble model requires solving $11 \cdot 2 \cdot 4 \cdot 6 = 528$ nonlinear optimization problems for each location, which amounts to $18 \cdot 528 = 9504$ optimization problems for the European Hub Data Set.

Due to this high computational cost the *maximum number of iterations* within each optimization is an important hyperparameter that balances the trade-off between computational feasibilty and sufficient convergence of the iterative optimization algorithm.
Here, we ultimately settled with $10.000$ maximum steps which could ensure convergence with respect to a *tolerance level* of $10^{-8}$ in the vast majority of cases.

Finally, it is worth noting that the weight vector of the ensemble model $\mathbf{w}^*$ is learned on a *training set* such that a fair comparison with all individual post-processing methods on a separate *validation set* is possible.



## Comparison of CQR, QSA & Ensemble

Now that we have introduced *Conformalized Quantile Regression* in \Cref{cqr}, *Quantile Spread Averaging* in \Cref{qsa} and the *Ensemble* Model in \Cref{ensemble}, the obvious question is which of the methods performs best.
This section conducts a detailed comparison across various feature combinations.
Due to the high computational demands of Quantile Spread Averaging, we limit the discussion to the compact UK Covid-19 Forecasting Challenge data set.
The results that constitute the starting point of the analysis can be generated with the following commands:

```{r, eval=FALSE}
library(postforecasts)

df_updated <- uk_data |>
  update_predictions(
    methods = c(
      "cqr", "cqr_asymmetric", "qsa_uniform", "qsa_flexible", "qsa_flexible_symmetric"
    ),
    cv_init_training = 0.5
  ) |>
  collect_predictions() |>
  add_ensemble()
```

```{r, ch2-wis-comparison, echo=FALSE}
tab_training <- uk_complete |>
  extract_training_set() |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = "method") |>
  select(method:dispersion) |>
  rename(`training score` = interval_score)

tab_validation <- uk_complete |>
  extract_validation_set() |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = "method") |>
  select(method:dispersion) |>
  rename(`validation score` = interval_score)

tab_validation |>
  mutate(`training score` = tab_training$`training score`) |>
  relocate(`training score`, .after = `validation score`) |>
  arrange(`validation score`) |>
  display_table(
    caption = "WIS of all Post-Processing Methods on Training and Validation Set on UK Data"
  )
```

\Cref{tab:ch2-wis-comparison} collects the Weighted Interval Scores for each method on the training and validation set, sorted by increasing validation score.
There are a couple of interesting findings:

- All six custom methods improve out-of-sample performance compared to the original predictions on the UK data set.

- All three QSA versions lead to lower validation scores than any CQR variant. 
Thus, based on this first impression, the familiy of QSA post-processing methods clearly outperforms the CQR algorithm for the UK data.

- The ensemble model is the clear winner: 
Combining information from multiple QSA and CQR methods works better on new data than any individual method on its own.
This suggests that the five building block methods are not redundant in the sense that they have different strengths and weaknesses depending on the location in feature space.

- The asymmetric CQR method suffers most from *overfitting*.
Compared to the European Forecast Hub data, where overfitting was not a major issue as described in \Cref{cqr-asymmetric}, the more flexible CQR modification results in the lowest training but highest validation score for the small UK data set.

- In general, additional design *restrictions* such as identical weights in case of `qsa_uniform` and/or the symmetry assumption in case of `cqr` and `qsa_flexible_symmetric` have some kind of *regularization* effect which leads to better generalization to the validation set.
Indeed, the *least* flexible versions of both method frameworks indicate the best validation performance and yet, unsurprisingly, the highest training score.

- All methods improve the original forecasts by *expanding* the prediction intervals which is indicated by the larger *dispersion* values.
`qsa_flexible_symmetric` produces by far the widest intervals on average, yet we can not observe a correlation of better validation scores and narrower or wider prediction intervals. 

```{r, include=FALSE}
weights_df <- attr(uk_complete, which = "weights")

weights_only <- do.call("rbind", weights_df$weights) |>
  magrittr::set_colnames(value = c(
    "cqr",  "cqr_asymmetric", "qsa_uniform", "qsa_flexible_symmetric",
    "qsa_flexible"
  )) |>
  as_tibble()

weights_full <- bind_cols(
  weights_df |> select(-c(location, weights)),
  weights_only
) |>
  filter(quantile < 0.5) |> 
  rowwise() |>
  mutate(rowsum = sum(c_across(cols = cqr:qsa_flexible))) |>
  mutate(max_weight = max(c_across(cols = cqr:qsa_flexible))) |>
  ungroup()
```

```{r, ch2-weight-thresholds, echo=FALSE}
weights_full |>
  summarise(
    `> 0.5` = mean(max_weight > 0.5),
    `> 0.9` = mean(max_weight > 0.9),
    `> 0.99` = mean(max_weight > 0.99),
  ) |>
  display_table(caption = paste(
    "Fraction of Feature Combinations where largest",
    "Ensemble Weight exceeds Threshold"
  ))
```

\Cref{tab:ch2-wis-comparison} convincingly demonstrates that the ensemble model leads to the best forecasts. 
Thus, we want to gain more insight how the ensemble predictions are created for this specific use case.
Recall that the weights for each of the five building block methods are by construction nonnegative and (in case of convergence) sum to one.
A different set of weights is computed for each of the $6 \cdot 2 \cdot 4 \cdot 11 = 528$ combinations of `model`, `target_type`, `horizon` and `quantile` (pairs).
One question of interest is if the optimization algorithm tends more towards evenly distributed weights within each combination by assigning a positive weight to many component methods, or rather selects a single winning method with a weight of $1$ and all remaining methods are discarded with a weight of $0$.

\Cref{tab:ch2-largest-weights} provides a first insight into the weight distribution of the ensemble model.
In $97$% of all feature combinations a single method has a larger weight than all of the competing methods combined.
Further, in more than half of the optimization solutions the ensemble emulates one particular method by concentrating the entire weight mass on a single point.
Hence, although we can not observe a strict *winner takes it all* procedure, the weight distribution is heavily skewed towards one contributing component at each location in feature space.

```{r, ch2-largest-weights, echo=FALSE}
weights_full |>
  rowwise() |>
  mutate(best_method = which.max(c_across(cqr:qsa_flexible))) |>
  mutate(best_method = case_when(
    best_method == 1 ~ "cqr",
    best_method == 2 ~ "cqr_asymmetric",
    best_method == 3 ~ "qsa_uniform",
    best_method == 4 ~ "qsa_flexible_symmetric",
    best_method == 5 ~ "qsa_flexible"
  )) |>
  group_by(best_method) |>
  summarise(
    num_best = n(),
    frac_best = num_best / nrow(weights_full)
  ) |>
  tidyr::pivot_longer(cols = num_best:frac_best) |>
  tidyr::pivot_wider(names_from = best_method, values_from = value) |>
  select(cqr, cqr_asymmetric, qsa_uniform, qsa_flexible_symmetric, qsa_flexible) |>
  display_table(
    caption = paste(
      "Number (Row 1) and Fraction (Row 2)",
      "of largest Ensemble Weights for each Method"
    )
  )
```

Now that we have discovered that there seems to be a single method that clearly outperforms its competition for most covariate combinations, we want to find out *which* of the five post-processing methods takes the winning trophy most often.
\Cref{tab:ch2-largest-weights} displays the frequency with which the ensemble assigns the largest weight to each method.
The first row contains the absolute number of times and the second row the fraction of all $528$ optimization problems where the methods contributed most to the ensemble model.

In more than $90$% of cases the largest weight is given to either the asymmetric CQR or the flexible QSA method whereas the uniform QSA method almost never has the largest impact on the ensemble.
This finding is particularly interesting in comparison with \Cref{tab:ch2-wis-comparison}: Since the ensemble is fitted on the *training* set, it distributes the weights according to the training set performance of each individual method.
`cqr_asymmetric` and `qsa_flexible` indeed have the best training scores whereas `qsa_uniform` performs worst on the training set which exactly corresponds to the order of \Cref{tab:ch2-largest-weights}.
With this connection in mind it seems even more surprising that the ensemble method generalizes very well to out-of-sample data while simultaneously rewarding potentially overfitting methods like `cqr_asymmetric` during its own learning process.

```{r, include=FALSE}
# find rows for plot where ensemble weights are distributed and algorithm converged (sum
# of weights close to 1)
weights_full |>
  filter(model == "seabbs", rowsum > 0.99, max_weight < 0.8)
```

```{r, echo=FALSE}
mod <- "seabbs"
t <- "Cases"
h <- 4
q <- 0.025

plot_weights <- weights_full |>
  filter(model == mod, target_type == t, horizon == h, quantile == q) |>
  mutate(across(.cols = cqr:qsa_flexible, .fns = ~ round(.x, digits = 4)))

plot_df <- uk_complete |>
  mutate(method = case_when(
    method == "cqr" ~ stringr::str_glue(
      "cqr\nweight: {plot_weights$cqr}"
    ),
    method == "cqr_asymmetric" ~ stringr::str_glue(
      "cqr_asymmetric\nweight: {plot_weights$cqr_asymmetric}"
    ),
    method == "qsa_uniform" ~ stringr::str_glue(
      "qsa_uniform\nweight: {plot_weights$qsa_uniform}"
    ),
    method == "qsa_flexible_symmetric" ~ stringr::str_glue(
      "qsa_flexible_symmetric\nweight: {plot_weights$qsa_flexible_symmetric}"
    ),
    method == "qsa_flexible" ~ stringr::str_glue(
      "qsa_flexible\nweight: {plot_weights$qsa_flexible}"
    ),
    TRUE ~ method
  ))

plot_intervals(
  plot_df,
  model = mod, target_type = t, quantile = q, horizon = h, base_size = 8
) +
  scale_color_manual(
    values = c("#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00", "#e6ab02", "#a65628")
  )
```


```{r, echo=FALSE}
df_method_model <- eval_methods(uk_complete, summarise_by = "model")
p1 <- plot_eval(df_method_model, base_size = 7) +
  labs(y = NULL, title = "Method Comparison by Model and Target Type", subtitle = NULL) +
  theme(axis.text.x = element_blank())

df_method_target_type <- eval_methods(uk_complete, summarise_by = "target_type")
p2 <- plot_eval(df_method_target_type, base_size = 7) +
  labs(y = NULL, title = NULL, subtitle = NULL)

p1 / p2
```

```{r, echo=FALSE}
df_method_horizon <- eval_methods(uk_complete, summarise_by = "horizon")
p1 <- plot_eval(df_method_horizon, base_size = 7) +
  labs(title = "Method Comparison by Horizon and Quantile", subtitle = NULL) +
  theme(axis.text.x = element_blank())

b <- "black"
t <- "transparent"

df_method_quantile <- eval_methods(uk_complete, summarise_by = "quantile")
p2 <- plot_eval(df_method_quantile, base_size = 7) +
  labs(title = NULL, subtitle = NULL) +
  theme(
    axis.text.y = element_text(color = c(b, rep(c(t, b), 11)))
  )

p1 / p2
```
