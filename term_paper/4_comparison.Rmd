---
output:
  bookdown::pdf_document2:
    includes:
      in_header: preamble.tex
    toc: FALSE
    highlight: tango
    number_sections: TRUE
    df_print: tibble
    latex_engine: pdflatex
bibliography: [paper.bib, packages.bib]
biblio-style: apalike
urlcolor: black
linkcolor: blue
links-as-notes: true
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE, fig.align = "center",
  out.width = "100%", dpi = 300, fig.asp = 0.618, collapse = TRUE
)

devtools::load_all(".")
library(dplyr)
library(ggplot2)
library(patchwork)

uk_complete <- readr::read_rds(here::here("data_results", "uk_complete.rds"))
```

```{r, include=FALSE}
display_table <- function(df, caption, bold_header = TRUE,
                          striped = FALSE) {
  tab <- df |>
    kableExtra::kbl(
      digits = 2, align = "c", booktabs = TRUE, caption = caption
    ) |>
    kableExtra::row_spec(row = 0, bold = bold_header) |>
    kableExtra::kable_styling(position = "center", full_width = FALSE)

  if (striped) {
    tab <- tab |> kableExtra::kable_styling(latex_options = "striped")
  }

  return(tab)
}
```

# Method Comparison {#comparison}

This chapter aims to compare the effectiveness of all Post-Processing methods that were introduced in the previous chapters.
In particular, we investigate if some methods consistently outperform other procedures across a wide range of scenarios, i.e. different data sets and different covariate combinations.

Further, it will be interesting to observe the *types* of adjustments to the original forecasts:
Some methods might improve the Weighted Interval Score by *extending* the interval width and thus increasing coverage whereas others might yield a similar final score by *shrinking* the prediction intervals leading to a higher precision.
One can imagine even more variations: Moving the interval bounds farther apart or closer together can happen in *symmetric* or *asymmetric* manner and the interval's midpoint might stay *fixed* or get *shifted* throughout the post-processing process.

Before jumping into the analysis, we propose one additional model that, in contrast to those we have covered so far, does not add any new information to the equation.
Instead, it *combines* the predictions from existing post-processing methods to build an *ensemble* prediction.
The idea is that leveraging information from multiple independent algorithms can stabilize estimation since the ensemble learns to focus on a model with a strong performance for one particular covariate set while the same model might perform worse for a different covariate set and, thus, make little contributions to the ensemble in that case.

Next, we explain the mathematical reasoning behind the ensemble model in more detail.

## Ensemble Model {#ensemble}

There exist various options how to combine multiple building blocks into one ensemble.
We chose an approach that can be efficiently computed by well-understood algorithms on the one hand and is highly interpretable on the other hand.
Each quantile prediction of our ensemble model is a *convex combination* of the individual methods, i.e. a linear combination where all weights are contained in the unit interval and sum up to one.
Hence, the resulting value lives on the same scale as the original predictions and each weight can be interpreted as the *fractional contribution* of each building block method 

Consider one particular feature combination of `model`, `location`, `horizon`, `target_type` and `quantile`.
Let $n$ specify the number of observations in the training set within this combination, $\mathbf{y} \in \mathbb{R}^n$ the vector of true values, $\mathbf{l}_1, \ldots, \mathbf{l}_k \in \mathbb{R}^n$ vectors of original lower quantile predictions and $\mathbf{u}_1, \ldots, \mathbf{u}_k \in \mathbb{R}^n$ vectors of original upper quantile predictions from $k$ different post-processing procedures.

Then, for each such combination, the ensemble model computes weights $\mathbf{w}^* \in [0, 1]^k$ by solving the following nonlinear constrained optimization problem:
<!--  -->
$$
\begin{aligned}
\mathbf{w}^*
= \operatorname*{arg\,min}_{ \mathbf{w} \in [0, 1]^k} IS_\alpha(\mathbf{y})
&= \operatorname*{arg\,min}_{ \mathbf{w} \in [0, 1]^k} (\mathbf{u}-\mathbf{l}) + \frac{2}{\alpha} \cdot (\mathbf{l}-\mathbf{y}) \cdot \mathbbm{1} (\mathbf{y} \leq \mathbf{l}) + \frac{2}{\alpha} \cdot (\mathbf{y}-\mathbf{u}) \cdot \mathbbm{1}(\mathbf{y} \geq \mathbf{u}), \\
\text{with} \qquad \mathbf{l} &= \sum_{j=1}^{k} w_j \mathbf{l}_j, \;\; \mathbf{u} = \sum_{j=1}^{k} w_j \mathbf{u}_j \\
\text{s.t.} \qquad \left \Vert \mathbf{w} \right \Vert_1 &= \sum_{j=1}^{k} w_j = 1,
\end{aligned}
$$
<!--  -->
where all operations for vector inputs $\mathbf{l}$, $\mathbf{u}$ and $\mathbf{y}$ are understood elementwise and the *same* weights $w_j$, $j = 1, \ldots, k$ are chosen for lower and upper quantiles.  

Hence, we choose the (nonlinear) Interval Score (\Cref{wis}) as our objective function that we minimize subject to linear constraints.
The optimization step is implemented with the [nloptr](https://cran.r-project.org/web/packages/nloptr/index.html) package [@R-nloptr], which describes itself as "an R interface to NLopt, a free/open-source library for nonlinear optimization".

Note that, technically, the weight vector has to be denoted by $\mathbf{w}_{m, l, h, t, q}^*$ since the computed weights are generally different for each feature combination.
We omit the subscripts at this point to keep the notation clean.

The Interval Score always considers *pairs* of quantiles $\alpha$ and $1 - \alpha$ as outer bounds of a $(1 - 2 \alpha) \cdot 100\%$ prediction interval.
The best results are achieved when a separate weight vector for each quantile pair is computed.
Since our data sets contain $11$ quantile pairs, $2$ target types and $4$ horizons and we consider $6$ different forecasters, the ensemble model requires solving $11 \cdot 2 \cdot 4 \cdot 6 = 528$ nonlinear optimization problems for each location, which amounts to $18 \cdot 528 = 9504$ optimization problems for the European Hub Data Set.

Due to this high computational cost the *maximum number of iterations* within each optimization is an important hyperparameter that balances the trade-off between computational feasibilty and sufficient convergence of the iterative optimization algorithm.
Here, we ultimately settled with $10.000$ maximum steps which could ensure convergence with respect to a *tolerance level* of $10^{-8}$ in the vast majority of cases.

Finally, it is worth noting that the weight vector of the ensemble model $\mathbf{w}^*$ is learned on a *training set* such that a fair comparison with all individual post-processing methods on a separate *validation set* is possible.



## Comparison of CQR, QSA & Ensemble

Now that we have introduced *Conformalized Quantile Regression* in \Cref{cqr}, *Quantile Spread Averaging* in \Cref{qsa} and the *Ensemble* Model in \Cref{ensemble}, the obvious question is which of the methods performs best.
Thus, this section is dedicated to a detailed comparison across various covariate combinations as well as both the UK Forecasting Challenge data and the European Forecast Hub data.

Except for some minor modifications for computational efficiency, the results that constitute the starting point of the analysis in this chapter can be generated with the following commands, where `df` represents either the UK or the European Forecast Hub data set:

```{r, eval=FALSE}
library(postforecasts)

df_updated <- df |>
  update_predictions(
    methods = c(
      "cqr", "cqr_asymmetric", "qsa_uniform", "qsa_flexible", "qsa_flexible_symmetric"
    ),
    cv_init_training = 0.5
  ) |>
  collect_predictions() |>
  add_ensemble()
```

```{r, echo=FALSE}
tab_training <- uk_complete |>
  extract_training_set() |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = "method") |>
  select(method:dispersion) |>
  rename(`training score` = interval_score)

tab_validation <- uk_complete |>
  extract_validation_set() |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = "method") |>
  select(method:dispersion) |>
  rename(`validation score` = interval_score)

tab_validation |>
  mutate(`training score` = tab_training$`training score`) |>
  relocate(`training score`, .after = `validation score`) |>
  arrange(`validation score`) |>
  display_table(
    caption = "WIS of all Post-Processing Methods on Training and Validation Set"
  )
```

```{r, include=FALSE}
weights_df <- attr(uk_complete, which = "weights")

weights_only <- do.call("rbind", weights_df$weights) |>
  magrittr::set_colnames(value = c(
    "cqr",  "cqr_asymmetric", "qsa_uniform", "qsa_flexible_symmetric",
    "qsa_flexible"
  )) |>
  as_tibble()

weights_full <- bind_cols(
  weights_df |> select(-c(location, weights)),
  weights_only
) |>
  rowwise() |>
  mutate(rowsum = sum(c_across(cols = cqr:qsa_flexible))) |>
  mutate(max_weight = max(c_across(cols = cqr:qsa_flexible))) |>
  ungroup()
```

```{r, ch2-weight-thresholds, echo=FALSE}
weights_full |>
  summarise(
    `> 0.5` = mean(max_weight > 0.5),
    `> 0.9` = mean(max_weight > 0.9),
    `> 0.99` = mean(max_weight > 0.99),
  ) |>
  display_table(caption = paste(
    "Fraction of Feature Combinations where largest",
    "Ensemble Weight exceeds Threshold"
  ))
```

```{r, ch2-largest-weights, echo=FALSE}
weights_full |>
  rowwise() |>
  mutate(best_method = which.max(c_across(cqr:qsa_flexible))) |>
  mutate(best_method = case_when(
    best_method == 1 ~ "cqr",
    best_method == 2 ~ "cqr_asymmetric",
    best_method == 3 ~ "qsa_uniform",
    best_method == 4 ~ "qsa_flexible_symmetric",
    best_method == 5 ~ "qsa_flexible"
  )) |>
  group_by(best_method) |>
  summarise(
    num_best = n(),
    frac_best = num_best / nrow(weights_full)
  ) |>
  tidyr::pivot_longer(cols = num_best:frac_best) |>
  tidyr::pivot_wider(names_from = best_method, values_from = value) |>
  select(cqr, cqr_asymmetric, qsa_uniform, qsa_flexible_symmetric, qsa_flexible) |>
  display_table(
    caption = paste(
      "Number (Row 1) and Fraction (Row 2)",
      "of largest Ensemble Weights for each Method"
    )
  )
```

```{r, include=FALSE}
# find rows for plot where ensemble weights are distributed and algorithm converged (sum
# of weights close to 1)
weights_full |>
  filter(model == "seabbs", rowsum > 0.99, max_weight < 0.8)
```

```{r, echo=FALSE}
mod <- "seabbs"
t <- "Cases"
h <- 4
q <- 0.1

plot_weights <- weights_full |>
  filter(model == mod, target_type == t, horizon == h, quantile == q) |>
  mutate(across(.cols = cqr:qsa_flexible, .fns = ~ round(.x, digits = 4)))

plot_df <- uk_complete |>
  mutate(method = case_when(
    method == "cqr" ~ stringr::str_glue(
      "cqr\nweight: {plot_weights$cqr}"
    ),
    method == "cqr_asymmetric" ~ stringr::str_glue(
      "cqr_asymmetric\nweight: {plot_weights$cqr_asymmetric}"
    ),
    method == "qsa_uniform" ~ stringr::str_glue(
      "qsa_uniform\nweight: {plot_weights$qsa_uniform}"
    ),
    method == "qsa_flexible_symmetric" ~ stringr::str_glue(
      "qsa_flexible_symmetric\nweight: {plot_weights$qsa_flexible_symmetric}"
    ),
    method == "qsa_flexible" ~ stringr::str_glue(
      "qsa_flexible\nweight: {plot_weights$qsa_flexible}"
    ),
    TRUE ~ method
  ))

plot_intervals(
  plot_df,
  model = mod, target_type = t, quantile = q, horizon = h
) +
  # guides(color = guide_legend(nrow = 1)) +
  scale_color_brewer(palette = "Dark2")
```


```{r, echo=FALSE}
df_method_model <- eval_methods(uk_complete, summarise_by = "model")
p1 <- plot_eval(df_method_model, base_size = 7) +
  labs(y = NULL, title = "Method Comparisons by Model and Target Type", subtitle = NULL) +
  theme(axis.text.x = element_blank())

df_method_target_type <- eval_methods(uk_complete, summarise_by = "target_type")
p2 <- plot_eval(df_method_target_type, base_size = 7) +
  labs(y = NULL, title = NULL, subtitle = NULL)

p1 / p2
```

```{r, echo=FALSE}
df_method_horizon <- eval_methods(uk_complete, summarise_by = "horizon")
p1 <- plot_eval(df_method_horizon, base_size = 7) +
  labs(title = "Method Comparisons by Horizon and Quantile", subtitle = NULL) +
  theme(axis.text.x = element_blank())

b <- "black"
t <- "transparent"

df_method_quantile <- eval_methods(uk_complete, summarise_by = "quantile")
p2 <- plot_eval(df_method_quantile, base_size = 7) +
  labs(title = NULL, subtitle = NULL) +
  theme(
    axis.text.y = element_text(color = c(b, rep(c(t, b), 11)))
  )

p1 / p2
```
