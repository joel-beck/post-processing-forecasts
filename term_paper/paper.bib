
@article{bassett1982,
  title = {An {{Empirical Quantile Function}} for {{Linear Models}} with | Operatornameiid {{Errors}}},
  author = {Bassett, Gilbert and Koenker, Roger},
  year = {1982},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {77},
  number = {378},
  pages = {407},
  issn = {01621459},
  doi = {10.2307/2287261},
  url = {https://www.jstor.org/stable/2287261?origin=crossref},
  urldate = {2022-02-28},
  langid = {english}
}

@article{bracher2021,
  title = {Evaluating Epidemic Forecasts in an Interval Format},
  author = {Bracher, Johannes and Ray, Evan L. and Gneiting, Tilmann and Reich, Nicholas G.},
  year = {2021},
  month = dec,
  journal = {PLOS Computational Biology},
  volume = {17},
  number = {2},
  pages = {e1008618},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008618},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008618},
  urldate = {2022-02-26},
  abstract = {For practical reasons, many forecasts of case, hospitalization, and death counts in the context of the current Coronavirus Disease 2019 (COVID-19) pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID-19 Forecast Hub (https://covid19forecasthub.org/). Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This article provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts in this format. Specifically, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a decomposition into a measure of sharpness and penalties for over- and underprediction.},
  langid = {english},
  keywords = {Binomials,COVID 19,Epidemiological methods and statistics,Forecasting,Instrument calibration,Pandemics,Probability distribution,Public and occupational health}
}

@article{Gnanvi2021,
author = {Gnanvi, Janyce Eunice and Salako, Kolawol{\'{e}} Val{\`{e}}re and Kotanmi, Ga{\"{e}}tan Brezesky and {Gl{\`{e}}l{\`{e}} Kaka{\"{i}}}, Romain},
doi = {10.1016/j.idm.2020.12.008},
issn = {24680427},
journal = {Infectious Disease Modelling},
keywords = {Accuracy,Pandemic,Precision,Predictions,Ratio,SARS-CoV-2},
pages = {258--272},
title = {{On the reliability of predictions on Covid-19 dynamics: A systematic and critical review of modelling techniques}},
volume = {6},
year = {2021},
url = {https://www.researchgate.net/profile/Romain-Lucas-Glele-Kakai/publication/348498543_On_the_reliability_of_predictions_on_Covid-19_dynamics_A_systematic_and_critical_review_of_modelling_techniques/links/6006a858a6fdccdcb8646146/On-the-reliability-of-predictions-on-Covid-19-dynamics-A-systematic-and-critical-review-of-modelling-techniques.pdf}
}

@article{gneiting2007,
  title = {Strictly {{Proper Scoring Rules}}, {{Prediction}}, and {{Estimation}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E},
  year = {2007},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {102},
  number = {477},
  pages = {359--378},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214506000001437},
  url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000001437},
  urldate = {2022-02-26},
  langid = {english}
}

@book{hyndman2021,
  title = {Forecasting: Principles and Practice},
  author = {Hyndman, Rob and Athanasopoulos, George},
  year = {2021},
  edition = {Third},
  publisher = {{OTexts}},
  url = {https://otexts.com/fpp3/}
}

@article{romano2019,
  title = {Conformalized {{Quantile Regression}}},
  author = {Romano, Yaniv and Patterson, Evan and Cand{\`e}s, Emmanuel J.},
  year = {2019},
  month = may,
  journal = {arXiv:1905.03222 [stat]},
  eprint = {1905.03222},
  eprinttype = {arxiv},
  primaryclass = {stat},
  url = {http://arxiv.org/abs/1905.03222},
  urldate = {2022-02-25},
  abstract = {Conformal prediction is a technique for constructing prediction intervals that attain valid coverage in finite samples, without making distributional assumptions. Despite this appeal, existing conformal methods can be unnecessarily conservative because they form intervals of constant or weakly varying length across the input space. In this paper we propose a new method that is fully adaptive to heteroscedasticity. It combines conformal prediction with classical quantile regression, inheriting the advantages of both. We establish a theoretical guarantee of valid coverage, supplemented by extensive experiments on popular regression datasets. We compare the efficiency of conformalized quantile regression to other conformal methods, showing that our method tends to produce shorter intervals.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning,Statistics - Methodology}
}

@article{rumack2021,
  title = {Recalibrating Probabilistic Forecasts of Epidemics},
  author = {Rumack, Aaron and Tibshirani, Ryan J. and Rosenfeld, Roni},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.06305 [cs]},
  eprint = {2112.06305},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2112.06305},
  urldate = {2022-02-25},
  abstract = {Distributional forecasts are important for a wide variety of applications, including forecasting epidemics. Often, forecasts are miscalibrated, or unreliable in assigning uncertainty to future events. We present a recalibration method that can be applied to a black-box forecaster given retrospective forecasts and observations, as well as an extension to make this method more effective in recalibrating epidemic forecasts. This method is guaranteed to improve calibration and log score performance when trained and measured in-sample. We also prove that the increase in expected log score of a recalibrated forecaster is equal to the entropy of the PIT distribution. We apply this recalibration method to the 27 influenza forecasters in the FluSight Network and show that recalibration reliably improves forecast accuracy and calibration. This method is effective, robust, and easy to use as a post-processing tool to improve epidemic forecasts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@misc{tibshirani2019,
  title = {Advances and {{Challenges}} in {{Conformal Inference}}},
  author = {Tibshirani, Ryan},
  year = {2019},
  address = {{Carnegie Mellon University}},
  url = {https://www.stat.cmu.edu/~ryantibs/talks/conformal-2019.pdf}
}

@misc{ukdata,
  title = {UK Covid-19 Crowd Forecasting Challenge},
  author = {EpiForecasts-Team},
  year = {2021},
  address = {{London School of Hygiene & Tropical Medicine, European Centre for Disease Control and Prevention (ECDC)}},
  url = {https://www.crowdforecastr.org/2021/05/11/uk-challenge/}
}

@misc{euhubdata,
  title = {European Covid-19 Forecast Hub},
  author = {EpiForecasts-Team},
  year = {2021},
  address = {{London School of Hygiene and Tropical Medicine, European Centre for Disease Control and Prevention (ECDC)}},
  url = {https://covid19forecasthub.eu/index.html}
}

@book{fletcher2013practical,
  title={Practical methods of optimization},
  author={Fletcher, Roger},
  year={2013},
  publisher={John Wiley \& Sons}
}

@article{byrd1995limited,
  title={A limited memory algorithm for bound constrained optimization},
  author={Byrd, Richard H and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
  journal={SIAM Journal on scientific computing},
  volume={16},
  number={5},
  pages={1190--1208},
  year={1995},
  publisher={SIAM}
}


